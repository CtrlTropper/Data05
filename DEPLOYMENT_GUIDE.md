# HÆ°á»›ng Dáº«n Triá»ƒn Khai Há»‡ Thá»‘ng Chatbot RAG + LLM Offline

HÆ°á»›ng dáº«n chi tiáº¿t triá»ƒn khai há»‡ thá»‘ng chatbot RAG + LLM hoÃ n toÃ n offline trÃªn Ubuntu.

## ðŸ“‹ **Má»¤C Lá»¤C**

1. [Chuáº©n bá»‹ mÃ´i trÆ°á»ng](#1-chuáº©n-bá»‹-mÃ´i-trÆ°á»ng)
2. [Chuáº©n bá»‹ model offline](#2-chuáº©n-bá»‹-model-offline)
3. [Cháº¡y backend FastAPI](#3-cháº¡y-backend-fastapi)
4. [Build frontend ReactJS](#4-build-frontend-reactjs)
5. [Káº¿t ná»‘i frontend â†” backend](#5-káº¿t-ná»‘i-frontend--backend)
6. [Cháº¡y toÃ n há»‡ thá»‘ng qua Docker Compose](#6-cháº¡y-toÃ n-há»‡-thá»‘ng-qua-docker-compose)
7. [Kiá»ƒm thá»­ trong mÃ´i trÆ°á»ng khÃ´ng cÃ³ internet](#7-kiá»ƒm-thá»­-trong-mÃ´i-trÆ°á»ng-khÃ´ng-cÃ³-internet)

---

## 1. **CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG**

### **1.1. CÃ i Ä‘áº·t Python 3.9+**

```bash
# Cáº­p nháº­t package list
sudo apt update

# CÃ i Ä‘áº·t Python 3.9 vÃ  pip
sudo apt install -y python3.9 python3.9-pip python3.9-venv python3.9-dev

# Táº¡o symbolic link
sudo ln -sf /usr/bin/python3.9 /usr/bin/python3
sudo ln -sf /usr/bin/python3.9 /usr/bin/python

# Kiá»ƒm tra version
python3 --version
pip3 --version
```

### **1.2. CÃ i Ä‘áº·t NodeJS 18+**

```bash
# CÃ i Ä‘áº·t NodeJS 18
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Kiá»ƒm tra version
node --version
npm --version
```

### **1.3. CÃ i Ä‘áº·t Docker vÃ  Docker Compose**

```bash
# CÃ i Ä‘áº·t Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# ThÃªm user vÃ o docker group
sudo usermod -aG docker $USER

# CÃ i Ä‘áº·t Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# Kiá»ƒm tra installation
docker --version
docker-compose --version

# Logout vÃ  login láº¡i Ä‘á»ƒ Ã¡p dá»¥ng group changes
```

### **1.4. CÃ i Ä‘áº·t CUDA (Náº¿u cÃ³ GPU)**

```bash
# Kiá»ƒm tra GPU
lspci | grep -i nvidia

# CÃ i Ä‘áº·t CUDA Toolkit 11.8
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda

# ThÃªm CUDA vÃ o PATH
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# Kiá»ƒm tra CUDA
nvcc --version
nvidia-smi
```

### **1.5. CÃ i Ä‘áº·t Dependencies**

```bash
# Táº¡o virtual environment
cd /path/to/your/project
python3 -m venv venv
source venv/bin/activate

# CÃ i Ä‘áº·t Python dependencies
pip install --upgrade pip
pip install fastapi==0.104.1
pip install uvicorn[standard]==0.24.0
pip install pydantic==2.5.0
pip install python-multipart==0.0.6
pip install sentence-transformers==2.2.2
pip install transformers==4.35.0
pip install torch==2.1.0
pip install faiss-cpu==1.7.4
pip install numpy==1.24.3
pip install PyPDF2==3.0.1
pip install python-docx==1.1.0
pip install aiofiles==23.2.1
pip install python-dotenv==1.0.0
pip install pydantic-settings==2.0.3

# CÃ i Ä‘áº·t thÃªm cho GPU (náº¿u cÃ³)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install faiss-gpu==1.7.4
pip install accelerate
pip install bitsandbytes

# CÃ i Ä‘áº·t NodeJS dependencies
cd frontend
npm install
cd ..
```

---

## 2. **CHUáº¨N Bá»Š MODEL OFFLINE**

### **2.1. Táº¡o cáº¥u trÃºc thÆ° má»¥c**

```bash
# Táº¡o thÆ° má»¥c cho models
mkdir -p models/embedding
mkdir -p models/llm
mkdir -p data/docs
mkdir -p data/faiss_index
mkdir -p data/metadata
mkdir -p uploads
```

### **2.2. Táº£i Embedding Model (intfloat/multilingual-e5-large)**

```bash
# Táº¡o script táº£i embedding model
cat > download_embedding.py << 'EOF'
import os
from sentence_transformers import SentenceTransformer
import torch

print("ðŸ“¥ Downloading multilingual-e5-large...")
print("This may take a while depending on your internet connection...")

try:
    # Táº£i model
    model = SentenceTransformer('intfloat/multilingual-e5-large')
    
    # LÆ°u model vÃ o thÆ° má»¥c local
    model.save('models/embedding')
    
    print("âœ… Embedding model downloaded and saved successfully!")
    print(f"Model saved to: {os.path.abspath('models/embedding')}")
    
    # Test model
    print("ðŸ§ª Testing model...")
    test_text = "This is a test sentence for embedding"
    embedding = model.encode(test_text)
    print(f"âœ… Test embedding generated: shape {embedding.shape}")
    
except Exception as e:
    print(f"âŒ Error downloading model: {e}")
EOF

# Cháº¡y script táº£i model
python download_embedding.py
```

### **2.3. Táº£i LLM Model (gpt-oss-20b)**

```bash
# Táº¡o script táº£i LLM model
cat > download_llm.py << 'EOF'
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ðŸ“¥ Downloading gpt-oss-20b...")
print("This may take a while depending on your internet connection...")
print("Model size: ~40GB")

try:
    model_name = "gpt-oss-20b"  # Hoáº·c Ä‘Æ°á»ng dáº«n model cá»¥ thá»ƒ
    
    # Táº£i tokenizer
    print("Downloading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Táº£i model
    print("Downloading model (this will take a long time)...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )
    
    # LÆ°u model vÃ  tokenizer
    print("Saving model and tokenizer...")
    tokenizer.save_pretrained('models/llm')
    model.save_pretrained('models/llm')
    
    print("âœ… LLM model downloaded and saved successfully!")
    print(f"Model saved to: {os.path.abspath('models/llm')}")
    
    # Test model
    print("ðŸ§ª Testing model...")
    test_prompt = "Hello, how are you?"
    inputs = tokenizer(test_prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            temperature=0.7,
            do_sample=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"âœ… Test generation successful!")
    print(f"Input: {test_prompt}")
    print(f"Output: {response}")
    
except Exception as e:
    print(f"âŒ Error downloading model: {e}")
    print("Please check your internet connection and try again.")
EOF

# Cháº¡y script táº£i model
python download_llm.py
```

### **2.4. Kiá»ƒm tra models Ä‘Ã£ táº£i**

```bash
# Kiá»ƒm tra embedding model
ls -la models/embedding/
python -c "
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('models/embedding', local_files_only=True)
print('âœ… Embedding model loaded successfully')
"

# Kiá»ƒm tra LLM model
ls -la models/llm/
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('models/llm', local_files_only=True)
print('âœ… LLM model loaded successfully')
"
```

---

## 3. **CHáº Y BACKEND FASTAPI**

### **3.1. Cáº¥u hÃ¬nh Backend**

```bash
# Táº¡o file .env cho backend
cat > backend/.env << 'EOF'
# Backend Configuration
APP_NAME=RAG + LLM Chatbot API
APP_VERSION=1.0.0
DEBUG=True

# Paths
DATA_DIR=data
DOCS_DIR=data/docs
FAISS_INDEX_DIR=data/faiss_index
FAISS_STORE_DIR=data/faiss_store
METADATA_DIR=data/metadata
UPLOADS_DIR=uploads

# Model paths
MODELS_DIR=models
EMBEDDING_MODEL_PATH=models/embedding
LLM_MODEL_PATH=models/llm

# Embedding settings
EMBEDDING_CHUNK_SIZE=500
EMBEDDING_CHUNK_OVERLAP=50

# LLM settings
LLM_MAX_TOKENS=1000
LLM_TEMPERATURE=0.7
LLM_TOP_K=5

# Document metadata file
DOCUMENTS_METADATA_FILE=data/metadata/documents_metadata.json
FAISS_METADATA_FILE=data/metadata/faiss_metadata.json

# Model cache directory
MODEL_CACHE_DIR=models/cache
EOF
```

### **3.2. Cháº¡y Backend**

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c backend
cd backend

# KÃ­ch hoáº¡t virtual environment
source ../venv/bin/activate

# Cháº¡y backend
python main.py

# Hoáº·c sá»­ dá»¥ng uvicorn
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### **3.3. Kiá»ƒm tra Backend**

```bash
# Test health check
curl http://localhost:8000/api/health

# Test API documentation
# Má»Ÿ browser: http://localhost:8000/docs
```

---

## 4. **BUILD FRONTEND REACTJS**

### **4.1. Cáº¥u hÃ¬nh Frontend**

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c frontend
cd frontend

# CÃ i Ä‘áº·t dependencies
npm install

# Táº¡o file .env cho frontend
cat > .env << 'EOF'
VITE_API_BASE_URL=http://localhost:8000/api
VITE_APP_NAME=RAG Chatbot
VITE_APP_VERSION=1.0.0
EOF
```

### **4.2. Build Frontend**

```bash
# Build cho production
npm run build

# Hoáº·c cháº¡y development server
npm run dev
```

### **4.3. Kiá»ƒm tra Frontend**

```bash
# Test development server
# Má»Ÿ browser: http://localhost:5173

# Test production build
npx serve -s dist -l 3000
# Má»Ÿ browser: http://localhost:3000
```

---

## 5. **Káº¾T Ná»I FRONTEND â†” BACKEND**

### **5.1. Cáº¥u hÃ¬nh CORS**

```bash
# Kiá»ƒm tra CORS trong backend/main.py
# Äáº£m báº£o cÃ³:
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production nÃªn giá»›i háº¡n
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### **5.2. Test káº¿t ná»‘i**

```bash
# Test API tá»« frontend
curl -X GET http://localhost:8000/api/health

# Test upload document
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@test_document.txt"

# Test chat
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "Hello, how are you?", "top_k": 5}'
```

### **5.3. Kiá»ƒm tra Frontend-Backend Integration**

```bash
# Má»Ÿ browser vÃ  test:
# 1. Upload document
# 2. Generate embeddings
# 3. Chat vá»›i document
# 4. Test streaming response
```

---

## 6. **CHáº Y TOÃ€N Há»† THá»NG QUA DOCKER COMPOSE**

### **6.1. Táº¡o Dockerfile cho Backend**

```bash
cat > backend/Dockerfile << 'EOF'
FROM python:3.9-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p data/docs data/faiss_index data/metadata uploads models/embedding models/llm

# Expose port
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF
```

### **6.2. Táº¡o Dockerfile cho Frontend**

```bash
cat > frontend/Dockerfile << 'EOF'
# Build stage
FROM node:18-alpine as build

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy source code
COPY . .

# Build the application
RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built files from build stage
COPY --from=build /app/dist /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/nginx.conf

# Expose port
EXPOSE 80

# Start nginx
CMD ["nginx", "-g", "daemon off;"]
EOF
```

### **6.3. Táº¡o nginx.conf cho Frontend**

```bash
cat > frontend/nginx.conf << 'EOF'
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    server {
        listen 80;
        server_name localhost;

        root /usr/share/nginx/html;
        index index.html;

        # Handle client-side routing
        location / {
            try_files $uri $uri/ /index.html;
        }

        # Proxy API requests to backend
        location /api/ {
            proxy_pass http://backend:8000/api/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
EOF
```

### **6.4. Táº¡o docker-compose.yml**

```bash
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  backend:
    build: ./backend
    container_name: rag-chatbot-backend
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
      - ./uploads:/app/uploads
    environment:
      - PYTHONPATH=/app
      - DEBUG=True
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build: ./frontend
    container_name: rag-chatbot-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  models:
    driver: local
  data:
    driver: local
EOF
```

### **6.5. Cháº¡y Docker Compose**

```bash
# Build vÃ  cháº¡y containers
docker-compose up --build -d

# Kiá»ƒm tra status
docker-compose ps

# Xem logs
docker-compose logs -f

# Stop containers
docker-compose down
```

### **6.6. Kiá»ƒm tra Docker Deployment**

```bash
# Test backend
curl http://localhost:8000/api/health

# Test frontend
curl http://localhost:80

# Má»Ÿ browser: http://localhost
```

---

## 7. **KIá»‚M THá»¬ TRONG MÃ”I TRÆ¯á»œNG KHÃ”NG CÃ“ INTERNET**

### **7.1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng offline**

```bash
# Táº¯t internet connection
sudo ifconfig eth0 down
# Hoáº·c
sudo systemctl stop NetworkManager

# Kiá»ƒm tra khÃ´ng cÃ³ internet
ping google.com
# Should fail
```

### **7.2. Test Backend Offline**

```bash
# Cháº¡y backend
cd backend
source ../venv/bin/activate
python main.py

# Test health check
curl http://localhost:8000/api/health

# Test model loading
curl http://localhost:8000/api/chat/stats
```

### **7.3. Test Frontend Offline**

```bash
# Cháº¡y frontend
cd frontend
npm run dev

# Má»Ÿ browser: http://localhost:5173
# Test upload document
# Test chat functionality
```

### **7.4. Test Docker Offline**

```bash
# Cháº¡y Docker Compose
docker-compose up -d

# Test services
curl http://localhost:8000/api/health
curl http://localhost:80

# Má»Ÿ browser: http://localhost
```

### **7.5. Test toÃ n bá»™ workflow offline**

```bash
# 1. Upload document
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@test_document.txt"

# 2. Generate embeddings
curl -X POST http://localhost:8000/api/embed/document/{doc_id}

# 3. Test chat
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this document about?", "top_k": 5}'

# 4. Test streaming chat
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"question": "Explain the main points", "top_k": 3}'
```

### **7.6. Performance testing offline**

```bash
# Test response time
time curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "Test question", "top_k": 5}'

# Test memory usage
docker stats

# Test concurrent requests
for i in {1..10}; do
  curl -X POST http://localhost:8000/api/chat \
    -H "Content-Type: application/json" \
    -d '{"question": "Test question ' $i '", "top_k": 5}' &
done
wait
```

---

## ðŸŽ¯ **KIá»‚M TRA CUá»I CÃ™NG**

### **Checklist triá»ƒn khai**

- [ ] Python 3.9+ Ä‘Ã£ cÃ i Ä‘áº·t
- [ ] NodeJS 18+ Ä‘Ã£ cÃ i Ä‘áº·t
- [ ] Docker vÃ  Docker Compose Ä‘Ã£ cÃ i Ä‘áº·t
- [ ] CUDA Ä‘Ã£ cÃ i Ä‘áº·t (náº¿u cÃ³ GPU)
- [ ] Embedding model Ä‘Ã£ táº£i vá» `models/embedding/`
- [ ] LLM model Ä‘Ã£ táº£i vá» `models/llm/`
- [ ] Backend cháº¡y Ä‘Æ°á»£c trÃªn port 8000
- [ ] Frontend build vÃ  cháº¡y Ä‘Æ°á»£c
- [ ] Frontend káº¿t ná»‘i Ä‘Æ°á»£c vá»›i backend
- [ ] Docker Compose cháº¡y Ä‘Æ°á»£c
- [ ] Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng offline hoÃ n toÃ n

### **Test cuá»‘i cÃ¹ng**

```bash
# 1. Test toÃ n bá»™ há»‡ thá»‘ng
curl http://localhost:8000/api/health
curl http://localhost:80

# 2. Test upload vÃ  chat
# Má»Ÿ browser: http://localhost
# Upload document â†’ Generate embeddings â†’ Chat

# 3. Test offline
# Táº¯t internet â†’ Test láº¡i toÃ n bá»™ workflow

# 4. Test performance
# Test vá»›i nhiá»u documents vÃ  queries
```

---

## ðŸš€ **Káº¾T LUáº¬N**

Há»‡ thá»‘ng chatbot RAG + LLM offline Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng vá»›i:

- âœ… **HoÃ n toÃ n offline**: KhÃ´ng cáº§n internet khi cháº¡y
- âœ… **GPU optimized**: Sá»­ dá»¥ng CUDA cho performance tá»‘t
- âœ… **Docker ready**: Triá»ƒn khai dá»… dÃ ng vá»›i Docker Compose
- âœ… **Production ready**: Error handling vÃ  monitoring
- âœ… **Scalable**: CÃ³ thá»ƒ má»Ÿ rá»™ng theo nhu cáº§u

Há»‡ thá»‘ng sáºµn sÃ ng cho production vá»›i kháº£ nÄƒng hoáº¡t Ä‘á»™ng hoÃ n toÃ n offline!
