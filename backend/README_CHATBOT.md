# Chatbot Module - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

Module chatbot RAG + LLM ho·∫°t ƒë·ªông ho√†n to√†n offline v·ªõi gpt-oss-20b.

## üéØ **T√çNH NƒÇNG CH√çNH**

### **1. LLM Service**
- ‚úÖ Load model `gpt-oss-20b` t·ª´ `./models/llm/` (offline)
- ‚úÖ H√†m `generate_answer(question, context)` ‚Üí tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi
- ‚úÖ Streaming response support
- ‚úÖ Configurable generation parameters

### **2. RAG Chat Flow**
- ‚úÖ **Input**: question + doc_id (optional)
- ‚úÖ **Search**: T√¨m ki·∫øm context t·ª´ FAISS
- ‚úÖ **Generate**: T·∫°o c√¢u tr·∫£ l·ªùi v·ªõi LLM
- ‚úÖ **Response**: Tr·∫£ v·ªÅ answer + sources

### **3. API Endpoints**
- ‚úÖ `POST /api/chat` - Chat c∆° b·∫£n
- ‚úÖ `POST /api/chat/document/{doc_id}` - Chat v·ªõi document c·ª• th·ªÉ
- ‚úÖ `POST /api/chat/stream` - Streaming chat
- ‚úÖ `GET /api/chat/stats` - Th·ªëng k√™ h·ªá th·ªëng

## üöÄ **C√ÄI ƒê·∫∂T MODEL**

### **1. T·∫£i model gpt-oss-20b**

```bash
# T·∫°o th∆∞ m·ª•c
mkdir -p models/llm

# T·∫£i model (c·∫ßn c√≥ internet l·∫ßn ƒë·∫ßu)
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = 'gpt-oss-20b'  # Ho·∫∑c ƒë∆∞·ªùng d·∫´n model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.save_pretrained('models/llm')
model.save_pretrained('models/llm')
print('Model downloaded successfully!')
"
```

### **2. Ki·ªÉm tra model**

```bash
# Ki·ªÉm tra th∆∞ m·ª•c model
ls -la models/llm/
# N√™n c√≥ c√°c file: config.json, pytorch_model.bin, tokenizer.json, etc.
```

## üìö **API ENDPOINTS**

### **1. Basic Chat**
```http
POST /api/chat
Content-Type: application/json

{
    "question": "Tr√≠ tu·ªá nh√¢n t·∫°o l√† g√¨?",
    "top_k": 5,
    "max_tokens": 1000,
    "temperature": 0.7
}
```

**Response:**
```json
{
    "response": "Tr√≠ tu·ªá nh√¢n t·∫°o (AI) l√† m·ªôt lƒ©nh v·ª±c...",
    "sources": [
        {
            "content": "Tr√≠ tu·ªá nh√¢n t·∫°o l√†...",
            "similarity_score": 0.95,
            "document_id": "doc_uuid",
            "filename": "document.txt",
            "chunk_index": 1
        }
    ],
    "processing_time": 2.5,
    "question": "Tr√≠ tu·ªá nh√¢n t·∫°o l√† g√¨?",
    "doc_id": null
}
```

### **2. Document Chat**
```http
POST /api/chat/document/{document_id}
Content-Type: application/json

{
    "question": "H√£y t√≥m t·∫Øt n·ªôi dung ch√≠nh",
    "top_k": 3
}
```

### **3. Streaming Chat**
```http
POST /api/chat/stream
Content-Type: application/json

{
    "question": "Gi·∫£i th√≠ch v·ªÅ machine learning",
    "top_k": 5
}
```

**Response (Server-Sent Events):**
```
data: {"type": "start", "question": "Gi·∫£i th√≠ch v·ªÅ machine learning", "sources_count": 3}

data: {"type": "token", "content": "Machine"}

data: {"type": "token", "content": " learning"}

data: {"type": "token", "content": " l√†"}

data: {"type": "end"}
```

### **4. Chat Stats**
```http
GET /api/chat/stats
```

**Response:**
```json
{
    "llm_service": {
        "model_loaded": true,
        "model_path": "models/llm",
        "device": "cuda",
        "model_name": "gpt-oss-20b"
    },
    "embedding_service": {
        "model_loaded": true,
        "model_path": "models/embedding"
    },
    "faiss_store": {
        "total_vectors": 150,
        "documents_count": 5
    },
    "chat_capabilities": {
        "text_chat": true,
        "document_chat": true,
        "streaming_chat": true,
        "rag_enabled": true
    }
}
```

## üîß **S·ª¨ D·ª§NG TRONG CODE**

### **1. Basic Chat**
```python
from services.llm_service import llm_service

# T·∫°o c√¢u tr·∫£ l·ªùi v·ªõi context
question = "Tr√≠ tu·ªá nh√¢n t·∫°o l√† g√¨?"
context = "Tr√≠ tu·ªá nh√¢n t·∫°o l√† lƒ©nh v·ª±c khoa h·ªçc m√°y t√≠nh..."

answer = llm_service.generate_answer(question, context)
print(answer)
```

### **2. Chat v·ªõi RAG**
```python
from services.embedding_service import embedding_service
from services.llm_service import llm_service
from db.faiss_store import faiss_store

# Search context
search_results = faiss_store.search_text(
    query_text="machine learning",
    top_k=3,
    embedding_service=embedding_service
)

# Create context
context = "\n".join([r["content"] for r in search_results])

# Generate answer
answer = llm_service.generate_answer("Machine learning l√† g√¨?", context)
print(answer)
```

### **3. Streaming Response**
```python
# Stream response tokens
for token in llm_service.generate_answer_with_streaming(question, context):
    print(token, end="", flush=True)
```

## üöÄ **C√ÅCH S·ª¨ D·ª§NG**

### **1. Test Chatbot**
```bash
# Ch·∫°y test script
python test_chat.py
```

### **2. API Testing**
```bash
# Basic chat
curl -X POST "http://localhost:8000/api/chat" \
  -H "Content-Type: application/json" \
  -d '{"question": "Tr√≠ tu·ªá nh√¢n t·∫°o l√† g√¨?", "top_k": 5}'

# Document chat
curl -X POST "http://localhost:8000/api/chat/document/document_uuid" \
  -H "Content-Type: application/json" \
  -d '{"question": "T√≥m t·∫Øt n·ªôi dung ch√≠nh"}'

# Chat stats
curl "http://localhost:8000/api/chat/stats"
```

### **3. Workflow ho√†n ch·ªânh**
```bash
# 1. Upload document
curl -X POST "http://localhost:8000/api/documents/upload" \
  -F "file=@document.txt"

# 2. Create embeddings
curl -X POST "http://localhost:8000/api/embed/document/{doc_id}" \
  -H "Content-Type: application/json" \
  -d '{"chunk_size": 500, "chunk_overlap": 50}'

# 3. Chat with document
curl -X POST "http://localhost:8000/api/chat/document/{doc_id}" \
  -H "Content-Type: application/json" \
  -d '{"question": "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?"}'
```

## üìä **RAG FLOW**

### **1. Input Processing**
```
User Question ‚Üí Validation ‚Üí Text Cleaning
```

### **2. Context Retrieval**
```
Question ‚Üí Embedding ‚Üí FAISS Search ‚Üí Top-K Chunks
```

### **3. Context Formatting**
```
Chunks ‚Üí Context Creation ‚Üí Prompt Building
```

### **4. LLM Generation**
```
Prompt ‚Üí LLM ‚Üí Response ‚Üí Post-processing
```

### **5. Response Formatting**
```
Response + Sources ‚Üí JSON Response
```

## ‚ö° **PERFORMANCE**

### **Benchmarks**
- **Basic Chat**: ~2-5s (bao g·ªìm LLM generation)
- **RAG Chat**: ~3-8s (bao g·ªìm search + generation)
- **Streaming Chat**: ~1-3s (first token)
- **Document Chat**: ~2-6s (v·ªõi document filter)

### **T·ªëi ∆∞u h√≥a**
- ‚úÖ **Model Caching**: Load model m·ªôt l·∫ßn
- ‚úÖ **Batch Processing**: X·ª≠ l√Ω multiple requests
- ‚úÖ **Context Optimization**: Ch·ªâ l·∫•y top-k chunks
- ‚úÖ **Memory Management**: GPU memory cleanup

## üîç **C·∫§U H√åNH**

### **Generation Parameters**
```python
# C·∫≠p nh·∫≠t generation config
llm_service.update_generation_config(
    max_new_tokens=1500,
    temperature=0.8,
    top_p=0.95,
    top_k=50
)
```

### **Search Parameters**
```python
# C·∫•u h√¨nh search
search_params = {
    "top_k": 5,        # S·ªë chunks
    "doc_id": None,    # Document filter
    "temperature": 0.7 # LLM temperature
}
```

## üêõ **TROUBLESHOOTING**

### **1. Model kh√¥ng load ƒë∆∞·ª£c**
```bash
# Ki·ªÉm tra th∆∞ m·ª•c model
ls -la models/llm/

# Ki·ªÉm tra logs
tail -f logs/app.log
```

### **2. Chat response ch·∫≠m**
```bash
# Ki·ªÉm tra GPU usage
nvidia-smi

# Ki·ªÉm tra memory
htop
```

### **3. Kh√¥ng t√¨m th·∫•y context**
```bash
# Ki·ªÉm tra FAISS index
curl "http://localhost:8000/api/embed/stats"

# Ki·ªÉm tra documents
curl "http://localhost:8000/api/documents"
```

## üìù **NOTES**

- **Model**: gpt-oss-20b (20B parameters)
- **Context Length**: 2048 tokens
- **Generation**: Configurable parameters
- **RAG**: T√≠ch h·ª£p v·ªõi FAISS search
- **Streaming**: Server-Sent Events
- **Offline**: Ho√†n to√†n offline, kh√¥ng c·∫ßn internet
- **Memory**: C·∫ßn √≠t nh·∫•t 16GB RAM cho model 20B
- **GPU**: Khuy·∫øn ngh·ªã GPU v·ªõi √≠t nh·∫•t 12GB VRAM
