"""
Vector Service - T√≠ch h·ª£p Embedding v√† FAISS
Service ch√≠nh ƒë·ªÉ x·ª≠ l√Ω vector operations
"""

import os
import logging
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path

from .embedding_service import embedding_service
from ..db.faiss_store import faiss_store

logger = logging.getLogger(__name__)

class VectorService:
    def __init__(self):
        """
        Kh·ªüi t·∫°o Vector Service
        """
        self.embedding_service = embedding_service
        self.faiss_store = faiss_store
        self.is_initialized = False
        
        logger.info("Vector Service initialized")

    async def initialize(self):
        """
        Kh·ªüi t·∫°o c√°c services
        """
        try:
            logger.info("üöÄ Initializing Vector Service...")
            
            # Load embedding model
            await self.embedding_service.load_model()
            
            # Load FAISS index
            self.faiss_store.load_index()
            
            self.is_initialized = True
            logger.info("‚úÖ Vector Service initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error initializing Vector Service: {e}")
            raise

    async def cleanup(self):
        """
        Cleanup resources
        """
        try:
            logger.info("üßπ Cleaning up Vector Service...")
            
            # Save FAISS index
            self.faiss_store.save_index()
            
            # Cleanup embedding service
            await self.embedding_service.cleanup()
            
            self.is_initialized = False
            logger.info("‚úÖ Vector Service cleaned up")
            
        except Exception as e:
            logger.error(f"‚ùå Error cleaning up Vector Service: {e}")
            raise

    def add_document(self, 
                    text: str, 
                    doc_id: str, 
                    chunk_index: int = 0,
                    filename: str = "") -> str:
        """
        Th√™m m·ªôt document v√†o vector database
        
        Args:
            text: N·ªôi dung text
            doc_id: ID c·ªßa document
            chunk_index: Index c·ªßa chunk
            filename: T√™n file
            
        Returns:
            str: Chunk ID
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            chunk_id = self.faiss_store.add_document(
                text=text,
                doc_id=doc_id,
                chunk_index=chunk_index,
                filename=filename,
                embedding_service=self.embedding_service
            )
            
            logger.info(f"‚úÖ Added document chunk: {chunk_id}")
            return chunk_id
            
        except Exception as e:
            logger.error(f"‚ùå Error adding document: {e}")
            raise

    def add_document_chunks(self, 
                           chunks: List[str], 
                           doc_id: str,
                           filename: str = "") -> List[str]:
        """
        Th√™m nhi·ªÅu chunks c·ªßa document
        
        Args:
            chunks: Danh s√°ch text chunks
            doc_id: ID c·ªßa document
            filename: T√™n file
            
        Returns:
            List[str]: Danh s√°ch chunk IDs
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            chunk_ids = self.faiss_store.add_document_chunks(
                chunks=chunks,
                doc_id=doc_id,
                filename=filename,
                embedding_service=self.embedding_service
            )
            
            logger.info(f"‚úÖ Added {len(chunks)} chunks for document {doc_id}")
            return chunk_ids
            
        except Exception as e:
            logger.error(f"‚ùå Error adding document chunks: {e}")
            raise

    def search(self, 
               query: str, 
               top_k: int = 5, 
               doc_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        T√¨m ki·∫øm vectors t∆∞∆°ng t·ª±
        
        Args:
            query: Text query
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            doc_id: N·∫øu c√≥, ch·ªâ t√¨m trong document n√†y
            
        Returns:
            List[Dict]: K·∫øt qu·∫£ t√¨m ki·∫øm
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            results = self.faiss_store.search_text(
                query_text=query,
                top_k=top_k,
                doc_id=doc_id,
                embedding_service=self.embedding_service
            )
            
            logger.info(f"‚úÖ Search completed: {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error searching: {e}")
            raise

    def clear_doc(self, doc_id: str) -> bool:
        """
        X√≥a document kh·ªèi vector database
        
        Args:
            doc_id: ID c·ªßa document
            
        Returns:
            bool: True n·∫øu x√≥a th√†nh c√¥ng
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            success = self.faiss_store.clear_doc(doc_id)
            
            if success:
                logger.info(f"‚úÖ Cleared document: {doc_id}")
            else:
                logger.warning(f"‚ö†Ô∏è Failed to clear document: {doc_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Error clearing document: {e}")
            raise

    def get_document_chunks(self, doc_id: str) -> List[Dict[str, Any]]:
        """
        L·∫•y t·∫•t c·∫£ chunks c·ªßa document
        
        Args:
            doc_id: ID c·ªßa document
            
        Returns:
            List[Dict]: Danh s√°ch chunks
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            chunks = self.faiss_store.get_document_chunks(doc_id)
            logger.info(f"‚úÖ Retrieved {len(chunks)} chunks for document {doc_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå Error getting document chunks: {e}")
            raise

    def get_stats(self) -> Dict[str, Any]:
        """
        L·∫•y th·ªëng k√™ v·ªÅ vector database
        """
        try:
            embedding_stats = self.embedding_service.get_model_info()
            faiss_stats = self.faiss_store.get_stats()
            
            stats = {
                "embedding_service": embedding_stats,
                "faiss_store": faiss_stats,
                "vector_service": {
                    "initialized": self.is_initialized,
                    "total_vectors": faiss_stats.get("total_vectors", 0),
                    "total_documents": faiss_stats.get("total_documents", 0)
                }
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Error getting stats: {e}")
            return {"error": str(e)}

    def backup(self, backup_path: str):
        """
        Backup vector database
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            self.faiss_store.backup(backup_path)
            logger.info(f"‚úÖ Backed up vector database to {backup_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Error backing up: {e}")
            raise

    def clear_all(self):
        """
        X√≥a t·∫•t c·∫£ d·ªØ li·ªáu
        """
        if not self.is_initialized:
            raise RuntimeError("Vector Service not initialized. Call initialize() first.")
        
        try:
            self.faiss_store.clear_all()
            logger.info("‚úÖ Cleared all vector data")
            
        except Exception as e:
            logger.error(f"‚ùå Error clearing all data: {e}")
            raise

    def validate_query(self, query: str) -> bool:
        """
        Validate query text
        """
        if not query or not query.strip():
            return False
        
        if len(query.strip()) < 2:
            return False
        
        return True

    def chunk_text(self, 
                   text: str, 
                   chunk_size: int = 500, 
                   chunk_overlap: int = 50) -> List[str]:
        """
        Chia text th√†nh chunks
        
        Args:
            text: Text c·∫ßn chia
            chunk_size: K√≠ch th∆∞·ªõc chunk
            chunk_overlap: ƒê·ªô ch·ªìng l·∫•p gi·ªØa c√°c chunks
            
        Returns:
            List[str]: Danh s√°ch chunks
        """
        if not text or not text.strip():
            return []
        
        text = text.strip()
        
        # N·∫øu text ng·∫Øn h∆°n chunk_size, tr·∫£ v·ªÅ to√†n b·ªô
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # N·∫øu kh√¥ng ph·∫£i chunk cu·ªëi, t√¨m v·ªã tr√≠ c·∫Øt ph√π h·ª£p
            if end < len(text):
                # T√¨m d·∫•u c√¢u g·∫ßn nh·∫•t
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if text[i] in '.!?\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # T√≠nh start cho chunk ti·∫øp theo
            start = end - chunk_overlap
            if start >= len(text):
                break
        
        logger.info(f"‚úÖ Chunked text into {len(chunks)} chunks")
        return chunks

# Global instance
vector_service = VectorService()
