"""
Data Initialization Service
T·ª± ƒë·ªông load v√† embedding d·ªØ li·ªáu ban ƒë·∫ßu khi kh·ªüi ƒë·ªông h·ªá th·ªëng
"""

import os
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

class DataInitializationService:
    """Service kh·ªüi t·∫°o d·ªØ li·ªáu ban ƒë·∫ßu"""
    
    def __init__(self):
        self.data_dir = "data"
        self.categories = {
            "Luat": "Luat",
            "TaiLieuTiengViet": "TaiLieuTiengViet", 
            "TaiLieuTiengAnh": "TaiLieuTiengAnh",
            "Uploads": "uploads"
        }
        self.supported_extensions = ['.pdf', '.txt', '.md', '.docx']
        self.is_initialized = False
        self.embedding_service = None
        self.vector_service = None
        self.pdf_processor = None
        
    async def initialize(self, embedding_service, vector_service, pdf_processor):
        """Kh·ªüi t·∫°o service v·ªõi dependencies"""
        try:
            self.embedding_service = embedding_service
            self.vector_service = vector_service
            self.pdf_processor = pdf_processor
            
            # T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt
            await self._create_directories()
            
            # Load d·ªØ li·ªáu ban ƒë·∫ßu
            await self._load_initial_data()
            
            self.is_initialized = True
            logger.info("‚úÖ Data Initialization Service initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error initializing Data Initialization Service: {e}")
            raise
    
    async def _create_directories(self):
        """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
        try:
            # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
            os.makedirs(self.data_dir, exist_ok=True)
            
            # T·∫°o c√°c th∆∞ m·ª•c category
            for category in self.categories.values():
                category_path = os.path.join(self.data_dir, category)
                os.makedirs(category_path, exist_ok=True)
                logger.info(f"üìÅ Created directory: {category_path}")
            
            # T·∫°o th∆∞ m·ª•c uploads
            uploads_path = os.path.join(self.data_dir, "uploads")
            os.makedirs(uploads_path, exist_ok=True)
            logger.info(f"üìÅ Created directory: {uploads_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Error creating directories: {e}")
            raise
    
    async def _load_initial_data(self):
        """Load v√† embedding d·ªØ li·ªáu ban ƒë·∫ßu"""
        try:
            logger.info("üîÑ Starting initial data loading...")
            
            total_documents = 0
            total_chunks = 0
            
            # Load t·ª´ng category
            for category_name, category_path in self.categories.items():
                if category_name == "Uploads":
                    continue  # Skip uploads folder
                
                category_full_path = os.path.join(self.data_dir, category_path)
                
                if os.path.exists(category_full_path):
                    logger.info(f"üìö Loading documents from category: {category_name}")
                    
                    documents_processed, chunks_created = await self._process_category(
                        category_full_path, 
                        category_name
                    )
                    
                    total_documents += documents_processed
                    total_chunks += chunks_created
                    
                    logger.info(f"‚úÖ Category {category_name}: {documents_processed} documents, {chunks_created} chunks")
                else:
                    logger.warning(f"‚ö†Ô∏è Category directory not found: {category_full_path}")
            
            logger.info(f"üéâ Initial data loading completed: {total_documents} documents, {total_chunks} chunks")
            
        except Exception as e:
            logger.error(f"‚ùå Error loading initial data: {e}")
            raise
    
    async def _process_category(self, category_path: str, category_name: str) -> tuple[int, int]:
        """
        X·ª≠ l√Ω t·∫•t c·∫£ t√†i li·ªáu trong m·ªôt category
        
        Args:
            category_path: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c category
            category_name: T√™n category
            
        Returns:
            tuple[int, int]: (s·ªë documents, s·ªë chunks)
        """
        try:
            documents_processed = 0
            chunks_created = 0
            
            # T√¨m t·∫•t c·∫£ files trong category
            files = self._find_documents(category_path)
            
            if not files:
                logger.info(f"üìÇ No documents found in category: {category_name}")
                return 0, 0
            
            logger.info(f"üìÑ Found {len(files)} documents in category: {category_name}")
            
            # X·ª≠ l√Ω t·ª´ng file
            for file_path in files:
                try:
                    logger.info(f"üîÑ Processing document: {os.path.basename(file_path)}")
                    
                    # Extract text from document
                    text_content = await self._extract_text_from_file(file_path)
                    
                    if text_content and text_content.strip():
                        # Create document ID
                        doc_id = self._generate_document_id(file_path, category_name)
                        
                        # Add to vector store with category metadata
                        chunks = await self.vector_service.add_document(
                            text=text_content,
                            doc_id=doc_id,
                            metadata={
                                "category": category_name,
                                "filename": os.path.basename(file_path),
                                "file_path": file_path,
                                "source": "initial_data",
                                "processed_at": datetime.now().isoformat()
                            }
                        )
                        
                        documents_processed += 1
                        chunks_created += len(chunks)
                        
                        logger.info(f"‚úÖ Processed: {os.path.basename(file_path)} -> {len(chunks)} chunks")
                    else:
                        logger.warning(f"‚ö†Ô∏è No text extracted from: {os.path.basename(file_path)}")
                        
                except Exception as e:
                    logger.error(f"‚ùå Error processing document {file_path}: {e}")
                    continue
            
            return documents_processed, chunks_created
            
        except Exception as e:
            logger.error(f"‚ùå Error processing category {category_name}: {e}")
            return 0, 0
    
    def _find_documents(self, directory_path: str) -> List[str]:
        """
        T√¨m t·∫•t c·∫£ documents trong th∆∞ m·ª•c
        
        Args:
            directory_path: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c
            
        Returns:
            List[str]: Danh s√°ch ƒë∆∞·ªùng d·∫´n files
        """
        try:
            files = []
            directory = Path(directory_path)
            
            for ext in self.supported_extensions:
                pattern = f"**/*{ext}"
                found_files = list(directory.glob(pattern))
                files.extend([str(f) for f in found_files])
            
            return files
            
        except Exception as e:
            logger.error(f"‚ùå Error finding documents in {directory_path}: {e}")
            return []
    
    async def _extract_text_from_file(self, file_path: str) -> str:
        """
        Tr√≠ch xu·∫•t text t·ª´ file
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file
            
        Returns:
            str: Text content
        """
        try:
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.pdf':
                return await self._extract_text_from_pdf(file_path)
            elif file_ext in ['.txt', '.md']:
                return await self._extract_text_from_text_file(file_path)
            elif file_ext == '.docx':
                return await self._extract_text_from_docx(file_path)
            else:
                logger.warning(f"‚ö†Ô∏è Unsupported file type: {file_ext}")
                return ""
                
        except Exception as e:
            logger.error(f"‚ùå Error extracting text from {file_path}: {e}")
            return ""
    
    async def _extract_text_from_pdf(self, file_path: str) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ PDF"""
        try:
            if not self.pdf_processor:
                logger.warning("‚ö†Ô∏è PDF processor not available")
                return ""
            
            # Process PDF (auto-detect type)
            extracted_text, metadata = self.pdf_processor.process_pdf(file_path, force_ocr=False)
            return extracted_text
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting text from PDF {file_path}: {e}")
            return ""
    
    async def _extract_text_from_text_file(self, file_path: str) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ text file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
                
        except Exception as e:
            logger.error(f"‚ùå Error reading text file {file_path}: {e}")
            return ""
    
    async def _extract_text_from_docx(self, file_path: str) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ DOCX file"""
        try:
            # TODO: Implement DOCX text extraction
            # For now, return empty string
            logger.warning(f"‚ö†Ô∏è DOCX extraction not implemented yet: {file_path}")
            return ""
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting text from DOCX {file_path}: {e}")
            return ""
    
    def _generate_document_id(self, file_path: str, category: str) -> str:
        """
        T·∫°o document ID t·ª´ file path v√† category
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file
            category: Category name
            
        Returns:
            str: Document ID
        """
        try:
            filename = os.path.basename(file_path)
            # T·∫°o ID t·ª´ category v√† filename
            doc_id = f"{category}_{filename}_{hash(file_path) % 10000}"
            return doc_id
            
        except Exception as e:
            logger.error(f"‚ùå Error generating document ID: {e}")
            return f"{category}_{os.path.basename(file_path)}"
    
    async def add_uploaded_document(self, file_path: str, filename: str) -> Dict[str, Any]:
        """
        Th√™m document ƒë∆∞·ª£c upload v√†o vector store
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file
            filename: T√™n file
            
        Returns:
            Dict[str, Any]: Processing result
        """
        try:
            logger.info(f"üîÑ Adding uploaded document: {filename}")
            
            # Extract text
            text_content = await self._extract_text_from_file(file_path)
            
            if not text_content or not text_content.strip():
                return {
                    "success": False,
                    "error": "No text extracted from document"
                }
            
            # Create document ID
            doc_id = self._generate_document_id(file_path, "Uploads")
            
            # Add to vector store
            chunks = await self.vector_service.add_document(
                text=text_content,
                doc_id=doc_id,
                metadata={
                    "category": "Uploads",
                    "filename": filename,
                    "file_path": file_path,
                    "source": "user_upload",
                    "processed_at": datetime.now().isoformat()
                }
            )
            
            logger.info(f"‚úÖ Added uploaded document: {filename} -> {len(chunks)} chunks")
            
            return {
                "success": True,
                "doc_id": doc_id,
                "chunks_created": len(chunks),
                "text_length": len(text_content)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error adding uploaded document: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def get_category_stats(self) -> Dict[str, Any]:
        """
        L·∫•y th·ªëng k√™ v·ªÅ c√°c category
        
        Returns:
            Dict[str, Any]: Category statistics
        """
        try:
            stats = {}
            
            for category_name, category_path in self.categories.items():
                category_full_path = os.path.join(self.data_dir, category_path)
                
                if os.path.exists(category_full_path):
                    files = self._find_documents(category_full_path)
                    stats[category_name] = {
                        "path": category_full_path,
                        "document_count": len(files),
                        "exists": True
                    }
                else:
                    stats[category_name] = {
                        "path": category_full_path,
                        "document_count": 0,
                        "exists": False
                    }
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Error getting category stats: {e}")
            return {}
    
    async def reload_category(self, category_name: str) -> Dict[str, Any]:
        """
        Reload m·ªôt category c·ª• th·ªÉ
        
        Args:
            category_name: T√™n category
            
        Returns:
            Dict[str, Any]: Reload result
        """
        try:
            if category_name not in self.categories:
                return {
                    "success": False,
                    "error": f"Unknown category: {category_name}"
                }
            
            category_path = os.path.join(self.data_dir, self.categories[category_name])
            
            if not os.path.exists(category_path):
                return {
                    "success": False,
                    "error": f"Category directory not found: {category_path}"
                }
            
            logger.info(f"üîÑ Reloading category: {category_name}")
            
            # Clear existing documents from this category
            # TODO: Implement clear by category in vector service
            
            # Process category
            documents_processed, chunks_created = await self._process_category(
                category_path, 
                category_name
            )
            
            logger.info(f"‚úÖ Reloaded category {category_name}: {documents_processed} documents, {chunks_created} chunks")
            
            return {
                "success": True,
                "category": category_name,
                "documents_processed": documents_processed,
                "chunks_created": chunks_created
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error reloading category {category_name}: {e}")
            return {
                "success": False,
                "error": str(e)
            }

# Global data initialization service instance
data_initialization_service = DataInitializationService()
