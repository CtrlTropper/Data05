{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install numpy=1.24.0 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fd215",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e8a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng GPU: 2\n",
      "GPU 0: Tesla T4\n",
      "  Tổng VRAM: 14.57 GB\n",
      "GPU 1: NVIDIA GeForce GT 1030\n",
      "  Tổng VRAM: 1.94 GB\n",
      "\n",
      "Đang sử dụng: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# login(token=\"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\")\n",
    "\n",
    "print(f\"Số lượng GPU: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(\n",
    "        f\"  Tổng VRAM: {torch.cuda.get_device_properties(i).total_memory / 1024 / 1024**2:.2f} GB\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # Chỉ định GPU 0 (Tesla T4)\n",
    "print(f\"\\nĐang sử dụng: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c44850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  5 20:08:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GT 1030         Off |   00000000:18:00.0  On |                  N/A |\n",
      "| 35%   39C    P0             N/A /   30W |     649MiB /   2048MiB |     20%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   63C    P0             31W /   70W |     400MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2020      G   /usr/lib/xorg/Xorg                            286MiB |\n",
      "|    0   N/A  N/A      2708      G   /usr/bin/gnome-shell                           62MiB |\n",
      "|    0   N/A  N/A      3099      G   /opt/teamviewer/tv_bin/TeamViewer               5MiB |\n",
      "|    0   N/A  N/A   1124722      G   ...irefox/6565/usr/lib/firefox/firefox        201MiB |\n",
      "|    0   N/A  N/A   1125684      G   ...erProcess --variations-seed-version         85MiB |\n",
      "|    0   N/A  N/A   1130197      G   gnome-control-center                            1MiB |\n",
      "|    1   N/A  N/A      2020      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1157649      C   python                                        392MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 163616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7415b",
   "metadata": {},
   "source": [
    "## Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aff13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tesla/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tesla/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import copy\n",
    "\n",
    "# from googletrans import Translator\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58c878",
   "metadata": {},
   "source": [
    "## Load index và chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7911b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model từ ổ cứng\n",
    "embedding_model_path = \"./embedding/Vietnamese_Embedding\"\n",
    "embedding_model = SentenceTransformer(embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da73d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = \"./embedding/results/all_faiss.index\"\n",
    "faiss_index = faiss.read_index(faiss_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f66c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embedding/results/all_embeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "chunks = []\n",
    "for item in data:\n",
    "    chunks.extend(item[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b742",
   "metadata": {},
   "source": [
    "## Load mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fb16a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f99389f4b94ab99f44ec0deb96378d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(46305, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=46305, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Đường dẫn đến thư mục chứa model đã tải\n",
    "model_path = \"./models/Vinallama-7b-Chat\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Đặt token padding\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.to(device)  # Chuyển model sang GPU\n",
    "\n",
    "# Chuyển model sang chế độ đánh giá (không training)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49627284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned_path = \"result/vinallama-7b-chat-lora-finetuned\"\n",
    "\n",
    "# Load cấu hình PEFT để biết base model gốc là gì\n",
    "peft_config = PeftConfig.from_pretrained(model_finetuned_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load mô hình gốc (chưa fine-tune)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Gắn các tham số LoRA đã học vào mô hình gốc\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_finetuned_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_finetuned = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "# Tokenizer đã fine-tune sẽ giống hệt với tokenizer gốc\n",
    "\n",
    "# peft_model = PeftModel.from_pretrained(pretrain_model, model_finetuned_path)\n",
    "peft_model.to(device)\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sánh vocab size\n",
    "print(\"Vocab size equal:\", len(tokenizer) == len(tokenizer_finetuned))\n",
    "\n",
    "# So sánh vocab nội dung\n",
    "print(\"Same vocab:\", tokenizer.get_vocab() == tokenizer_finetuned.get_vocab())\n",
    "\n",
    "# So sánh token hóa\n",
    "sample = \"The quick brown fox\"\n",
    "print(\"Same encoding:\", tokenizer.encode(sample) == tokenizer_finetuned.encode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e54f32",
   "metadata": {},
   "source": [
    "## Sinh câu trả lời"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50403bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(query, top_k=3, max_tokens_per_chunk=512):\n",
    "    query_vector = embedding_model.encode([query])  # dùng model embedding đã dùng trước đó\n",
    "    D, I = faiss_index.search(np.array(query_vector).astype(\"float32\"), top_k)\n",
    "\n",
    "    context_chunks = []\n",
    "    for i in I[0]:\n",
    "        chunk = chunks[i]\n",
    "        tokens = tokenizer.tokenize(chunk)\n",
    "        if len(tokens) > max_tokens_per_chunk:\n",
    "            tokens = tokens[:max_tokens_per_chunk]\n",
    "            chunk = tokenizer.convert_tokens_to_string(tokens)\n",
    "        context_chunks.append(chunk.strip())\n",
    "\n",
    "    return context_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc31e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context_chunks, question):\n",
    "    context = \"\\n---\\n\".join(context_chunks)\n",
    "    return f\"\"\"\n",
    "    <|im_start|>system\n",
    "    Bạn là một trợ lý AI tuyển sinh của Học viện Kỹ thuật Quân sự. Chỉ trả lời người dùng dựa trên thông tin được cung cấp dưới đây. Nếu không biết, hãy trả lời: \"Tôi không có thông tin về câu hỏi này.\" Không được bịa.\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Thông tin:\n",
    "    {context}\n",
    "\n",
    "    Câu hỏi: {question}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5dfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_generation(llm_model):\n",
    "    generation_config = llm_model.generation_config\n",
    "    generation_config.max_new_tokens = 256  # đủ dài để trả lời rõ\n",
    "    generation_config.num_beams = 3         # tăng tính chính xác (tìm tốt hơn)\n",
    "    generation_config.early_stopping = True\n",
    "    generation_config.do_sample = False     # dùng beam search (ổn định, không ngẫu nhiên)\n",
    "    generation_config.num_return_sequences = 1\n",
    "\n",
    "    # Các tham số sau sẽ bị **bỏ qua** vì `do_sample = False`\n",
    "    # → bạn nên xóa hoặc để chú thích\n",
    "    generation_config.temperature = None\n",
    "    generation_config.top_p = 1.0\n",
    "\n",
    "    # Thêm 2 tham số giúp giảm lặp, sát nghĩa hơn:\n",
    "    # generation_config.repetition_penalty = 1.2\n",
    "    # generation_config.no_repeat_ngram_size = 3\n",
    "\n",
    "    return generation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8a9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, llm_model):\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = llm_model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=configure_generation(llm_model)\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82516cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query, llm_model, top_k=3):\n",
    "    context_chunks = get_relevant_chunks(query, top_k=top_k)\n",
    "    prompt = build_prompt(context_chunks, query)\n",
    "    answer = generate_answer(prompt, llm_model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab1a5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Trụ sở của học viện nằm ở đâu?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46269fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      "    Bạn là một trợ lý AI tuyển sinh của Học viện Kỹ thuật Quân sự. Chỉ trả lời người dùng dựa trên thông tin được cung cấp dưới đây. Nếu không biết, hãy trả lời: \"Tôi không có thông tin về câu hỏi này.\" Không được bịa.\n",
      "     \n",
      "    <|im_start|> user\n",
      "    Thông tin:\n",
      "    + Tháng 01 năm 2008, Học viện KTQS được Nhà nước công nhận là một trong 15 trường Đại học trọng điêm Quốc gia.\n",
      "b) Sứ mệnh Sứ mệnh của.\n",
      "Học viện KTQS là đào tạo nguồn nhân lực chất lượng cao, nghiên cứu phát triên, sản xuât chê thử, chuyên giao công nghệ tiên tiên và hội nhập quốc tê, góp phân đắc lực vào sự nghiệp xây dựng và bảo vệ Tổ quốc, phát triên ngành khoa học công nghệ quân sự Việt Nam.\n",
      "c) Tâm nhìn\n",
      "_ Đến năm 2030, Học viện KTQS trở thành trường Đại học nghiên cứu năm trong tốp đâu về Khoa học kỹ thuật và công nghệ của đât nước, có những lĩnh vực tương đương với các trường Đại học lớn trong khu M và hội nhập quốc tê, năm trong tôp 700 các trường Đại học tiên tiễn, hàng đâu trên thê giới; hoàn thành xuất sắc mọi nhiệm vụ được Đảng, Nhà nước và Quân đội giao cho.\n",
      "- Đến năm 2045, Học viện KTQS nằm trong tốp 500 các trường Đại học tiên tiên, hàng đâu trên thê giới.\n",
      "d) Thành tích tiêu biểu Với gần 60 năm phấn đấu và trưởng thành, Học viện KTQS đã vinh dự được Đảng, Nhà nước tặng thưởng:\n",
      "+ Danh hiệu Đơn vị Anh hùng LLVT nhân dân thời kỳ đôi mới.\n",
      "+ 01 Huân chương Hồ Chí Minh.\n",
      "+ 01 Huân chương Độc lập hạng Nhất.\n",
      "+01 Huân chương Độc lập hạng Nhì.\n",
      "+ 01 Huân chương Độc lập hạng Ba.\n",
      "+ 02 Huân chương Quân công hạng Nhất.\n",
      "+01 Huân chương Quân công hạng Nhì.\n",
      "+ 02 Huân chương Lao động hạng Ba.\n",
      "+ 03 Huân chương Chiến công: hạng Nhất, hạng Nhì, hạng Ba.\n",
      "2.\n",
      "Mã cơ sở đào tạo trong tuyến sinh: KQH (Mã tuyến sinh này sử dụng thông nhất cho tuyên sinh đào tạo đại học hệ quấn sự và dân sự).\n",
      "3.\n",
      "Trụ sở của Học viện: Trụ sở chính: Số 236, đường Hoàng Quốc Việt, Quận Bắc Từ Liêm, Thành phô Hà Nội: 4.\n",
      "Địa chỉ trang thông tin điện tử: http:/www.mfta.edu.vn; 5.\n",
      "Số điện thoại liên hệ: 069.5 15.2263 6.\n",
      "Địa chỉ công khai quy chế tuyển sinh, thông tin tuyển sinh: http:/www.mta.edu.vn; 7.\n",
      "---\n",
      "Số điện thoại liên hệ: 069.5 15.2263 6. Địa chỉ công khai quy chế tuyển sinh, thông tin tuyển sinh: http:/www.mta.edu.vn; 7. Các thông tin công khai về hoạt động của Học viện: Địa chỉ công khai thông tin: http://www.mta.edu.vn; 4) Ngành đào tạo Học viện tổ chức đảo tạo trình độ đại học với 15 ngành gắn với các lĩnh vực phát triên của khoa học và công nghệ nói chung và khoa học công nghệ quân sự nói riêng bao gồm: TT Tên ngành Ghi chú 1_ Kỹ thuật Cơ khí 7520103 2_Ì Kỹ thuật Điện tử - viễn thông 752007 3 Kỹ thuật Xây dựng 7580201 ] ˆ 4_ Công nghệ thông tin 748001 j 5 Công nghệ vật liệu 751002 ˆ 6 _ Công nghệ kỹ thuật hóa học 751001 7_ Kỹ thuật Cơ điện tử 752014 8 Khoa học máy tính 7480101 9_ Hệ thống thông tin 7480104 10 Kỹ thuật phần mềm 748013 11 Ì Kỹ thuật điều khiển và tự động hóa 72016 Mạng máy tính và truyền thông đữ 12 liệu 7480102\n",
      "(Mạng máy tính) l3 Kỹ thuật xây dựng công trình giao 7580205 Ố thông 14 An toàn thông tin 7480202 15 Kỹ thuật Trắc địa — bản đồ 7520503 b) Chương trình đào tạo Đối với đào tạo kỹ sư quân sự, Học viện tổ chức đảo tạo hơn 5Ø chương trình. Đông thời tỗ chức đào tạo 02 chương trình chất lượng cao An toàn thông tin, Thông tin; chương trình kỹ sư tài năng, và các chương trình tạo nguồn học tập tại nước ngoài.4\n",
      "Đối với sinh viên dân sự, năm 2025 Học viện đào tạo theo 10 chương trình đào fạo thuộc các ngành công nghệ mới, công nghệ chiến lược quốc gia. Sinh viên dân sự có cơ hội học tập theo các chương trình đào tạo hợp tác với các trường đào tạo nước ngoài như Đại học công nghệ Sydney, Australia\n",
      "(UT) và định hướng một số trường đại học khác như Đại học Dublin, Ailen\n",
      "(UCD)...\n",
      "---\n",
      "1. Tên cơ sở đào tạo: Học viện Kỹ thuật quân sự (MIilitary Technical Academy), tên dân sự là Trường Đại học Kỹ thuật Lê Quý Đôn (Le Quy Đon Technical University)\n",
      "\n",
      "    Câu hỏi: Trụ sở của học viện nằm ở đâu?\n",
      "     \n",
      "    <|im_start|> assistant\n",
      "Trụ sở của Học viện KTQS nằm tại số 236, đường Hoàng Quốc Việt, quận Bắc Từ Liêm, thành phố Hà Nội. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_answer(query, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_finetuned = rag_answer(query, peft_model)\n",
    "print(response_finetuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NguyenK56",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
