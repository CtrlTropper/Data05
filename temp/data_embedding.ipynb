{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772683d8",
   "metadata": {},
   "source": [
    "# Khai báo thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ccb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.6-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (2.9.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/tesla/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tesla/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2023.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain-community) (3.12.14)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tesla/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting openai<2.0.0,>=1.86.0 (from langchain-openai)\n",
      "  Downloading openai-1.96.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (3.7.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.86.0->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.86.0->langchain-openai)\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/tesla/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.1.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tesla/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tesla/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.69-py3-none-any.whl (441 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "Downloading openai-1.96.1-py3-none-any.whl (757 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.5/757.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.2/582.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.4.6-py3-none-any.whl (367 kB)\n",
      "Downloading orjson-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, python-dotenv, packaging, orjson, mypy-extensions, jsonpatch, jiter, httpx-sse, greenlet, distro, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, marshmallow, pydantic-settings, openai, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: packaging━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K    Found existing installation: packaging 23.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K    Uninstalling packaging-23.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K      Successfully uninstalled packaging-23.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.41 dataclasses-json-0.6.7 distro-1.9.0 greenlet-3.2.3 httpx-sse-0.4.1 jiter-0.10.0 jsonpatch-1.33 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.69 langchain-openai-0.3.28 langchain-text-splitters-0.3.8 langsmith-0.4.6 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.96.1 orjson-3.11.0 packaging-25.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64659f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tesla/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tesla/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path  # tiện hơn os.path\n",
    "from underthesea import sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import faiss\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288d890",
   "metadata": {},
   "source": [
    "# Load GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e6c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng GPU: 2\n",
      "GPU 0: Tesla T4\n",
      "  Tổng VRAM: 14.57 GB\n",
      "GPU 1: NVIDIA GeForce GT 1030\n",
      "  Tổng VRAM: 1.94 GB\n",
      "\n",
      "Đang sử dụng: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Số lượng GPU: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(\n",
    "        f\"  Tổng VRAM: {torch.cuda.get_device_properties(i).total_memory / 1024 / 1024**2:.2f} GB\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # Chỉ định GPU 0 (Tesla T4)\n",
    "print(f\"\\nĐang sử dụng: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e82ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 17 20:19:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GT 1030         Off |   00000000:18:00.0 Off |                  N/A |\n",
      "| 35%   35C    P8             N/A /   30W |     477MiB /   2048MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             34W /   70W |    2294MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1925      G   /usr/lib/xorg/Xorg                            285MiB |\n",
      "|    0   N/A  N/A      2531      G   /usr/bin/gnome-shell                           93MiB |\n",
      "|    0   N/A  N/A      2725      G   /opt/teamviewer/tv_bin/TeamViewer               7MiB |\n",
      "|    0   N/A  N/A      3094      G   ...irefox/6259/usr/lib/firefox/firefox         14MiB |\n",
      "|    0   N/A  N/A     12185      G   ...erProcess --variations-seed-version         57MiB |\n",
      "|    0   N/A  N/A     32837      G   gnome-control-center                            1MiB |\n",
      "|    0   N/A  N/A     52009      G   /snap/vlc/3777/usr/bin/vlc                      0MiB |\n",
      "|    0   N/A  N/A     52067      G   /snap/vlc/3777/usr/bin/vlc                      0MiB |\n",
      "|    1   N/A  N/A      1925      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3902834      C   ...iniconda3/envs/NguyenK56/bin/python       2286MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0653e",
   "metadata": {},
   "source": [
    "# Load model embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba58493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model từ ổ cứng\n",
    "model_path = \"./Vietnamese_Embedding\"\n",
    "model = SentenceTransformer(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36d70d",
   "metadata": {},
   "source": [
    "# Tạo embeddings từ PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c5974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn file PDF bạn muốn xử lý\n",
    "pdf_path = \"../data/file/CTKM_01.pdf\"\n",
    "output_dir = \"./results\"\n",
    "\n",
    "all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dbcd1",
   "metadata": {},
   "source": [
    "## Bước 1: Check xem file pdf đó đã được embedding chưa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd3d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pdf_embedded(path):\n",
    "    \"\"\"\n",
    "    Kiểm tra xem file PDF đã được embedding hay chưa,\n",
    "    dựa vào all_embeddings.pkl (danh sách các file đã xử lý).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        return False  # Chưa có dữ liệu chung => chắc chắn chưa nhúng gì\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    existing_pdf_names = {entry['pdf_name'] for entry in all_data}\n",
    "\n",
    "    return pdf_name in existing_pdf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23587728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 PDF này đã được embedding trước đó.\n"
     ]
    }
   ],
   "source": [
    "if is_pdf_embedded(pdf_path):\n",
    "    print(\"📌 PDF này đã được embedding trước đó.\")\n",
    "else:\n",
    "    print(\"🔄 PDF này chưa được embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087138e1",
   "metadata": {},
   "source": [
    "## Bước 2: OCR PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ad984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Tiền xử lý ảnh để cải thiện OCR\n",
    "    \"\"\"\n",
    "    if img.mode != 'L':\n",
    "        img = img.convert('L')\n",
    "\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(1.5)\n",
    "    img = img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d22f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdf_to_text(pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    OCR file PDF thành text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"📖 Đang OCR file: {pdf_path}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "\n",
    "        full_text = \"\"\n",
    "        ocr_config = r'--oem 3 --psm 6 -l vie'\n",
    "\n",
    "        for page_num in range(total_pages):\n",
    "            print(f\"🔄 Xử lý trang {page_num + 1}/{total_pages}...\")\n",
    "\n",
    "            page = doc.load_page(page_num)\n",
    "            matrix = fitz.Matrix(2.5, 2.5)\n",
    "            pix = page.get_pixmap(matrix=matrix)\n",
    "            img_data = pix.tobytes(\"png\")\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "            img = preprocess_image(img)\n",
    "\n",
    "            try:\n",
    "                page_text = pytesseract.image_to_string(img, config=ocr_config)\n",
    "                full_text += page_text.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Lỗi OCR trang {page_num + 1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        print(f\"✅ Hoàn thành OCR {total_pages} trang\")\n",
    "\n",
    "        # 🧠 Tạo tên file JSON theo tên file PDF\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_ocr.txt\")\n",
    "\n",
    "        # Tạo thư mục nếu chưa tồn tại\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Lưu file JSON\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"📄 Kết quả đã lưu vào: {output_path}\")\n",
    "\n",
    "        # Trả về danh sách các trang với nội dung\n",
    "        return full_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi OCR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4e149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Đang OCR file: ../data/file/TTTS2025.pdf\n",
      "🔄 Xử lý trang 1/22...\n",
      "🔄 Xử lý trang 2/22...\n",
      "🔄 Xử lý trang 3/22...\n",
      "🔄 Xử lý trang 4/22...\n",
      "🔄 Xử lý trang 5/22...\n",
      "🔄 Xử lý trang 6/22...\n",
      "🔄 Xử lý trang 7/22...\n",
      "🔄 Xử lý trang 8/22...\n",
      "🔄 Xử lý trang 9/22...\n",
      "🔄 Xử lý trang 10/22...\n",
      "🔄 Xử lý trang 11/22...\n",
      "🔄 Xử lý trang 12/22...\n",
      "🔄 Xử lý trang 13/22...\n",
      "🔄 Xử lý trang 14/22...\n",
      "🔄 Xử lý trang 15/22...\n",
      "🔄 Xử lý trang 16/22...\n",
      "🔄 Xử lý trang 17/22...\n",
      "🔄 Xử lý trang 18/22...\n",
      "🔄 Xử lý trang 19/22...\n",
      "🔄 Xử lý trang 20/22...\n",
      "🔄 Xử lý trang 21/22...\n",
      "🔄 Xử lý trang 22/22...\n",
      "✅ Hoàn thành OCR 22 trang\n",
      "📄 Kết quả đã lưu vào: ./results/TTTS2025/TTTS2025_ocr.txt\n"
     ]
    }
   ],
   "source": [
    "# Bước 1: OCR PDF\n",
    "raw_text = ocr_pdf_to_text(pdf_path, output_dir)\n",
    "if not raw_text:\n",
    "    print(\"❌ Không thể OCR file PDF. Vui lòng kiểm tra lại.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb815a",
   "metadata": {},
   "source": [
    "## Bước 3: Làm sạch text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Làm sạch một đoạn văn bản OCR (string)\n",
    "    \"\"\"\n",
    "    # Loại ký tự không mong muốn (giữ lại tiếng Việt, toán học, đơn vị)\n",
    "    text = re.sub(r'[^\\w\\s.,;:()\\[\\]?!\\\"\\'\\-–—…°%‰≥≤→←≠=+/*<>\\n\\r]', '', text)\n",
    "\n",
    "    # Xử lý lỗi xuống dòng giữa từ hoặc giữa câu\n",
    "    text = re.sub(r'-\\n', '', text)             # nối từ bị gạch nối xuống dòng\n",
    "    text = re.sub(r'\\n(?=\\w)', ' ', text)       # dòng xuống không hợp lý → nối câu\n",
    "\n",
    "    # Dấu chấm lặp vô nghĩa → ba chấm\n",
    "    text = re.sub(r'\\.{3,}', '...', text)\n",
    "\n",
    "    # Chuẩn hóa khoảng trắng\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)   # giữ ngắt đoạn\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)       # nhiều khoảng trắng → 1 dấu cách\n",
    "    text = re.sub(r' *\\n *', '\\n', text)      # bỏ khoảng trắng đầu/cuối dòng\n",
    "\n",
    "    # Lưu file\n",
    "    clean_text = text.strip()\n",
    "    \n",
    "    # 🧠 Tạo tên file JSON theo tên file PDF\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_clean.txt\")\n",
    "\n",
    "    # Tạo thư mục nếu chưa tồn tại\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Lưu file JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"📄 Kết quả đã lưu vào: {output_path}\")\n",
    "    \n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e87f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Làm sạch text...\n",
      "📄 Kết quả đã lưu vào: ./results/TTTS2025/TTTS2025_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: Làm sạch text\n",
    "print(\"🧹 Làm sạch text...\")\n",
    "cleaned_text = clean_text(raw_text, pdf_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81300c",
   "metadata": {},
   "source": [
    "## Bước 4: Chia thành chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7373f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sections(text):\n",
    "    \"\"\"\n",
    "    Tách text thành các phần theo tiêu đề kiểu I., 1., a)\n",
    "    \"\"\"\n",
    "    sections = re.split(r'\\n(?=(?:[IVXLCDM]+\\.)|(?:\\d+\\.)|(?:[a-z]\\)))', text)\n",
    "    return [s.strip() for s in sections if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80143812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_chunks_vi_tokenized_with_section(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Chia văn bản tiếng Việt thành các chunk dựa trên số token,\n",
    "    giữ nguyên cấu trúc section và câu.\n",
    "    \"\"\"\n",
    "    sections = split_sections(text)\n",
    "    all_chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        sentences = sent_tokenize(section)\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            num_tokens = len(tokenizer.tokenize(sentence))\n",
    "\n",
    "            if current_tokens + num_tokens > chunk_size:\n",
    "                chunk_text = '\\n'.join(current_chunk).strip()\n",
    "                all_chunks.append(chunk_text)\n",
    "\n",
    "                # Overlap bằng token\n",
    "                overlap_chunk = []\n",
    "                total = 0\n",
    "                for s in reversed(current_chunk):\n",
    "                    toks = len(tokenizer.tokenize(s))\n",
    "                    if total + toks > overlap:\n",
    "                        break\n",
    "                    overlap_chunk.insert(0, s)\n",
    "                    total += toks\n",
    "\n",
    "                current_chunk = overlap_chunk + [sentence]\n",
    "                current_tokens = total + num_tokens\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += num_tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            all_chunks.append(' '.join(current_chunk).strip())\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e79362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Chia text thành chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Đã tạo 56 chunks\n"
     ]
    }
   ],
   "source": [
    "# Bước 3: Chia thành chunks\n",
    "print(\"✂️ Chia text thành chunks...\")\n",
    "chunks = split_text_to_chunks_vi_tokenized_with_section(cleaned_text)\n",
    "print(f\"📝 Đã tạo {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af218cd6",
   "metadata": {},
   "source": [
    "## Bước 5: Tạo embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a0b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Tạo embeddings cho các text chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🔄 Tạo embeddings cho {len(chunks)} chunks...\")\n",
    "        embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "        print(f\"✅ Hoàn thành tạo embeddings\")\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi tạo embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c83d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Tạo embeddings cho 56 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37664222005e45ddb7ff157bb9679308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hoàn thành tạo embeddings\n"
     ]
    }
   ],
   "source": [
    "# Bước 4: Tạo embeddings\n",
    "embeddings = create_embeddings(chunks)\n",
    "if embeddings is None:\n",
    "    print(\"❌ Không thể tạo embeddings. Vui lòng kiểm tra lại.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba621b9",
   "metadata": {},
   "source": [
    "## Bước 6: Lưu embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6efd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(chunks, embeddings, pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Lưu embeddings và chunks vào file\n",
    "    \"\"\"\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    # Tạo thư mục nếu chưa tồn tại\n",
    "    os.makedirs(os.path.join(output_dir, pdf_name), exist_ok=True)\n",
    "\n",
    "    # Lưu dữ liệu\n",
    "    data = {\n",
    "        'pdf_name': pdf_name,\n",
    "        'chunks': chunks,\n",
    "        'embeddings': embeddings,\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Lưu embeddings (pickle)\n",
    "    pickle_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_embeddings.pkl\")\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    # Lưu chunks (text file)\n",
    "    chunks_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_chunks.txt\")\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"CHUNKS TỪ FILE: {pdf_name}.pdf\\n\")\n",
    "        f.write(f\"Tạo lúc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Tổng số chunks: {len(chunks)}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            f.write(f\"CHUNK {i}:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            f.write(chunk + \"\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\\n\")\n",
    "\n",
    "    # Lưu thông tin embeddings (text file)\n",
    "    embedding_info_path = os.path.join(\n",
    "        output_dir, pdf_name, f\"{pdf_name}_embedding_info.txt\")\n",
    "    with open(embedding_info_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"THÔNG TIN EMBEDDINGS: {pdf_name}.pdf\\n\")\n",
    "        f.write(f\"Tạo lúc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"📊 THỐNG KÊ:\\n\")\n",
    "        f.write(f\"- Tổng số chunks: {len(chunks)}\\n\")\n",
    "        f.write(f\"- Kích thước embeddings: {embeddings.shape}\\n\")\n",
    "        f.write(f\"- Kiểu dữ liệu: {embeddings.dtype}\\n\")\n",
    "        f.write(\n",
    "            f\"- Kích thước mỗi vector: {embeddings.shape[1]} dimensions\\n\\n\")\n",
    "\n",
    "        f.write(f\"📝 PREVIEW EMBEDDINGS (5 chunks đầu):\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "        for i in range(min(5, len(chunks))):\n",
    "            f.write(f\"\\nCHUNK {i+1}:\\n\")\n",
    "            f.write(f\"Text: {chunks[i][:100]}...\\n\")\n",
    "            f.write(\n",
    "                f\"Embedding vector (10 giá trị đầu): {embeddings[i][:10].tolist()}\\n\")\n",
    "            f.write(f\"Vector norm: {np.linalg.norm(embeddings[i]):.4f}\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    # 4️⃣ Lưu FAISS index\n",
    "    index_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_faiss.index\")\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # --- 🔁 Cập nhật FAISS chung ---\n",
    "    all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "    if os.path.exists(all_faiss_path):\n",
    "        index_all = faiss.read_index(all_faiss_path)\n",
    "    else:\n",
    "        index_all = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index_all.add(embeddings.astype(np.float32))\n",
    "    faiss.write_index(index_all, all_faiss_path)\n",
    "\n",
    "    # --- 🔁 Cập nhật pickle chung ---\n",
    "    all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")\n",
    "    if os.path.exists(all_pickle_path):\n",
    "        with open(all_pickle_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "    else:\n",
    "        all_data = []\n",
    "\n",
    "    all_data.append(data)\n",
    "\n",
    "    with open(all_pickle_path, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "\n",
    "    print(f\"💾 Đã lưu embeddings: {pickle_path}\")\n",
    "    print(f\"📄 Đã lưu chunks: {chunks_path}\")\n",
    "    print(f\"📊 Đã lưu thông tin embeddings: {embedding_info_path}\")\n",
    "    print(f\"📌 Đã lưu FAISS index: {index_path}\")\n",
    "    print(f\"🔁 Cập nhật FAISS chung: {all_faiss_path}\")\n",
    "    print(f\"📦 Cập nhật pickle chung: {all_pickle_path}\")\n",
    "\n",
    "    return pickle_path, index_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b07fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Đã lưu embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "📄 Đã lưu chunks: ./results/TTTS2025/TTTS2025_chunks.txt\n",
      "📊 Đã lưu thông tin embeddings: ./results/TTTS2025/TTTS2025_embedding_info.txt\n",
      "📌 Đã lưu FAISS index: ./results/TTTS2025/TTTS2025_faiss.index\n",
      "🔁 Cập nhật FAISS chung: ./results/all_faiss.index\n",
      "📦 Cập nhật pickle chung: ./results/all_embeddings.pkl\n",
      "\n",
      "🎉 HOÀN THÀNH!\n",
      "📊 Thống kê:\n",
      "   - Số chunks: 56\n",
      "   - Kích thước embedding: (56, 1024)\n",
      "   - File embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "   - File chunks: ./results/TTTS2025/TTTS2025_chunks.txt\n",
      "   - File thông tin: ./results/TTTS2025/TTTS2025_embedding_info.txt\n",
      "✅ Đường dẫn embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "✅ Đường dẫn FAISS index: ./results/TTTS2025/TTTS2025_faiss.index\n"
     ]
    }
   ],
   "source": [
    "# Bước 5: Lưu embeddings\n",
    "pickle_path, faiss_path = save_embeddings(chunks, embeddings, pdf_path, output_dir)\n",
    "\n",
    "print(f\"\\n🎉 HOÀN THÀNH!\")\n",
    "print(f\"📊 Thống kê:\")\n",
    "print(f\"   - Số chunks: {len(chunks)}\")\n",
    "print(f\"   - Kích thước embedding: {embeddings.shape}\")\n",
    "print(f\"   - File embeddings: {pickle_path}\")\n",
    "print(\n",
    "    f\"   - File chunks: {pickle_path.replace('_embeddings.pkl', '_chunks.txt')}\")\n",
    "print(\n",
    "    f\"   - File thông tin: {pickle_path.replace('_embeddings.pkl', '_embedding_info.txt')}\")\n",
    "\n",
    "# Lưu lại đường dẫn file .pkl để sử dụng sau\n",
    "print(f\"✅ Đường dẫn embeddings: {pickle_path}\")\n",
    "print(f\"✅ Đường dẫn FAISS index: {faiss_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9f62",
   "metadata": {},
   "source": [
    "## Xóa pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d772b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa thư mục chứa embeddings của file PDF\n",
    "def delete_pdf_folder(pdf_path, output_dir):\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    folder_path = os.path.join(output_dir, pdf_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"🗑️ Đã xoá thư mục: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Thư mục không tồn tại: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14416ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa khỏi all_embeddings.pkl\n",
    "def remove_from_all_embeddings(pdf_path, all_pickle_path):\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        print(\"⚠️ all_embeddings.pkl không tồn tại.\")\n",
    "        return\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    new_data = [entry for entry in all_data if entry['pdf_name'] != pdf_name]\n",
    "\n",
    "    if len(new_data) == len(all_data):\n",
    "        print(f\"❌ PDF '{pdf_name}' không có trong all_embeddings.pkl.\")\n",
    "    else:\n",
    "        with open(all_pickle_path, 'wb') as f:\n",
    "            pickle.dump(new_data, f)\n",
    "        print(f\"✅ Đã xoá '{pdf_name}' khỏi all_embeddings.pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d13b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa khỏi all_faiss.index\n",
    "def rebuild_faiss_from_pickle(all_pickle_path, all_faiss_path):\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        print(\"⚠️ all_embeddings.pkl không tồn tại.\")\n",
    "        return\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"⚠️ Không còn dữ liệu nào trong all_embeddings.pkl.\")\n",
    "        if os.path.exists(all_faiss_path):\n",
    "            os.remove(all_faiss_path)\n",
    "            print(\"🗑️ Đã xoá all_faiss.index rỗng.\")\n",
    "        return\n",
    "\n",
    "    # Gom tất cả vectors lại\n",
    "    all_vectors = np.vstack([entry['embeddings'] for entry in all_data]).astype(np.float32)\n",
    "    dim = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(all_vectors)\n",
    "    faiss.write_index(index, all_faiss_path)\n",
    "    print(f\"🔄 Đã xây lại all_faiss.index với {all_vectors.shape[0]} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pdf_folder_and_update(pdf_path, output_dir):\n",
    "    all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")\n",
    "    all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "    \n",
    "    delete_pdf_folder(pdf_path, output_dir)\n",
    "    remove_from_all_embeddings(pdf_path, all_pickle_path)\n",
    "    rebuild_faiss_from_pickle(all_pickle_path, all_faiss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fce571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã xoá 'CTKM_01' khỏi all_embeddings.pkl.\n",
      "🔄 Đã xây lại all_faiss.index với 56 vectors.\n"
     ]
    }
   ],
   "source": [
    "delete_pdf_folder_and_update(pdf_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbb63f",
   "metadata": {},
   "source": [
    "# Demo tìm kiếm với câu hỏi bất kỳ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6effc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử tìm kiếm với câu hỏi bất kỳ\n",
    "query = \"Trụ sở của trường nằm ở đâu?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e58a2a",
   "metadata": {},
   "source": [
    "## Dùng Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42997699",
   "metadata": {},
   "source": [
    "## Bước 1: Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "493169b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path):\n",
    "    \"\"\"\n",
    "    Tải embeddings từ file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(embeddings_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f\"📂 Đã tải embeddings từ: {embeddings_path}\")\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi tải embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618ea1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Đã tải embeddings từ: ./results/TTTS2025_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Tải lại dữ liệu embeddings đã tạo\n",
    "data = load_embeddings(pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11f0c8",
   "metadata": {},
   "source": [
    "## Bước 2: Tìm kiếm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec0e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_chunks(query, embeddings_data, top_k=3):\n",
    "    \"\"\"\n",
    "    Tìm các chunk tương tự với query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tạo embedding cho query\n",
    "        query_embedding = model.encode([query])\n",
    "\n",
    "        # Tính cosine similarity\n",
    "        similarities = np.dot(\n",
    "            embeddings_data['embeddings'], query_embedding.T).flatten()\n",
    "\n",
    "        # Lấy top_k kết quả\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'chunk': embeddings_data['chunks'][idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'index': idx\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi tìm kiếm: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05b86080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEMO TÌM KIẾM\n",
      "📚 PDF: TTTS2025\n",
      "📝 Số chunks: 56\n",
      "==================================================\n",
      "\n",
      "1. 🔍 Độ tương đồng: 0.3401\n",
      "📄 Nội dung: + Tháng 01 năm 2008, Học viện KTQS được Nhà nước công nhận là một trong 15 trường Đại học trọng điêm Quốc gia.\n",
      "b) Sứ mệnh Sứ mệnh của.\n",
      "Học viện KTQS là đào tạo nguồn nhân lực chất lượng cao, nghiên cứu phát triên, sản xuât chê thử, chuyên giao công nghệ tiên tiên và hội nhập quốc tê, góp phân đắc lự...\n",
      "\n",
      "2. 🔍 Độ tương đồng: 0.2782\n",
      "📄 Nội dung: 1. Tên cơ sở đào tạo: Học viện Kỹ thuật quân sự (MIilitary Technical Academy), tên dân sự là Trường Đại học Kỹ thuật Lê Quý Đôn (Le Quy Đon Technical University)...\n",
      "\n",
      "3. 🔍 Độ tương đồng: 0.2573\n",
      "📄 Nội dung: Số điện thoại liên hệ: 069.5 15.2263 6. Địa chỉ công khai quy chế tuyển sinh, thông tin tuyển sinh: http:/www.mta.edu.vn; 7. Các thông tin công khai về hoạt động của Học viện: Địa chỉ công khai thông tin: http://www.mta.edu.vn; 4) Ngành đào tạo Học viện tổ chức đảo tạo trình độ đại học với 15 ngành ...\n"
     ]
    }
   ],
   "source": [
    "results = find_similar_chunks(query, data, top_k=3)\n",
    "\n",
    "print(f\"\\n🔍 DEMO TÌM KIẾM\")\n",
    "print(f\"📚 PDF: {data['pdf_name']}\")\n",
    "print(f\"📝 Số chunks: {len(data['chunks'])}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# In kết quả\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. 🔍 Độ tương đồng: {result['similarity']:.4f}\")\n",
    "    print(f\"📄 Nội dung: {result['chunk'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc8236",
   "metadata": {},
   "source": [
    "## Dùng FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45b3f8",
   "metadata": {},
   "source": [
    "## Bước 1: Load FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4ce55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index(index_path):\n",
    "    print(f\"📂 Đang tải FAISS index từ: {index_path}\")\n",
    "    return faiss.read_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd87e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Đang tải FAISS index từ: ./results/TTTS2025_faiss.index\n"
     ]
    }
   ],
   "source": [
    "index = load_faiss_index(faiss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a5ebe",
   "metadata": {},
   "source": [
    "## Bước 2: Encode câu hỏi thành vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query):\n",
    "    return model.encode([query])  # trả về numpy array (1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c44eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = encode_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690934c8",
   "metadata": {},
   "source": [
    "## Bước 3: Truy vấn FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5615d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(index, query_vector, top_k=5):\n",
    "    D, I = index.search(query_vector.astype('float32'), top_k)\n",
    "    return I[0], D[0]  # indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "indices, distances = search_faiss(index, query_vector, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cdbac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Kết quả truy vấn:\n",
      "\n",
      "🔹 Kết quả 1: (score: 1.3199)\n",
      "+ Tháng 01 năm 2008, Học viện KTQS được Nhà nước công nhận là một trong 15 trường Đại học trọng điêm Quốc gia.\n",
      "b) Sứ mệnh Sứ mệnh của.\n",
      "Học viện KTQS là đào tạo nguồn nhân lực chất lượng cao, nghiên cứu phát triên, sản xuât chê thử, chuyên giao công nghệ tiên tiên và hội nhập quốc tê, góp phân đắc lực vào sự nghiệp xây dựng và bảo vệ Tổ quốc, phát triên ngành khoa học công nghệ quân sự Việt Nam.\n",
      "c) Tâm nhìn\n",
      "_ Đến năm 2030, Học viện KTQS trở thành trường Đại học nghiên cứu năm trong tốp đâu về Kho\n",
      "\n",
      "🔹 Kết quả 2: (score: 1.4436)\n",
      "1. Tên cơ sở đào tạo: Học viện Kỹ thuật quân sự (MIilitary Technical Academy), tên dân sự là Trường Đại học Kỹ thuật Lê Quý Đôn (Le Quy Đon Technical University)\n",
      "\n",
      "🔹 Kết quả 3: (score: 1.4854)\n",
      "Số điện thoại liên hệ: 069.5 15.2263 6. Địa chỉ công khai quy chế tuyển sinh, thông tin tuyển sinh: http:/www.mta.edu.vn; 7. Các thông tin công khai về hoạt động của Học viện: Địa chỉ công khai thông tin: http://www.mta.edu.vn; 4) Ngành đào tạo Học viện tổ chức đảo tạo trình độ đại học với 15 ngành gắn với các lĩnh vực phát triên của khoa học và công nghệ nói chung và khoa học công nghệ quân sự nói riêng bao gồm: TT Tên ngành Ghi chú 1_ Kỹ thuật Cơ khí 7520103 2_Ì Kỹ thuật Điện tử - viễn thông 7\n"
     ]
    }
   ],
   "source": [
    "print(\"📌 Kết quả truy vấn:\")\n",
    "for i, (idx, dist) in enumerate(zip(indices, distances), 1):\n",
    "    print(f\"\\n🔹 Kết quả {i}: (score: {dist:.4f})\")\n",
    "    print(chunks[idx][:500])  # in tối đa 500 ký tự"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NguyenK56",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
