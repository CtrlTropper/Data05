{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772683d8",
   "metadata": {},
   "source": [
    "# Khai bÃ¡o thÆ° viá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ccb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.6-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (2.9.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/tesla/.local/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/tesla/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Collecting packaging>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tesla/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /home/tesla/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2023.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tesla/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain-community) (3.12.14)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tesla/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting openai<2.0.0,>=1.86.0 (from langchain-openai)\n",
      "  Downloading openai-1.96.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (3.7.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.86.0->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.86.0->langchain-openai)\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/tesla/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/tesla/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.1.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tesla/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tesla/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tesla/miniconda3/envs/NguyenK56/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.69-py3-none-any.whl (441 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "Downloading openai-1.96.1-py3-none-any.whl (757 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m757.5/757.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m582.2/582.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.4.6-py3-none-any.whl (367 kB)\n",
      "Downloading orjson-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, python-dotenv, packaging, orjson, mypy-extensions, jsonpatch, jiter, httpx-sse, greenlet, distro, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, marshmallow, pydantic-settings, openai, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: packagingâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K    Found existing installation: packaging 23.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K    Uninstalling packaging-23.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K      Successfully uninstalled packaging-23.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/26\u001b[0m [python-dotenv]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/26\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.41 dataclasses-json-0.6.7 distro-1.9.0 greenlet-3.2.3 httpx-sse-0.4.1 jiter-0.10.0 jsonpatch-1.33 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.69 langchain-openai-0.3.28 langchain-text-splitters-0.3.8 langsmith-0.4.6 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.96.1 orjson-3.11.0 packaging-25.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64659f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tesla/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tesla/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path  # tiá»‡n hÆ¡n os.path\n",
    "from underthesea import sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import faiss\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288d890",
   "metadata": {},
   "source": [
    "# Load GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e6c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ lÆ°á»£ng GPU: 2\n",
      "GPU 0: Tesla T4\n",
      "  Tá»•ng VRAM: 14.57 GB\n",
      "GPU 1: NVIDIA GeForce GT 1030\n",
      "  Tá»•ng VRAM: 1.94 GB\n",
      "\n",
      "Äang sá»­ dá»¥ng: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Sá»‘ lÆ°á»£ng GPU: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(\n",
    "        f\"  Tá»•ng VRAM: {torch.cuda.get_device_properties(i).total_memory / 1024 / 1024**2:.2f} GB\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # Chá»‰ Ä‘á»‹nh GPU 0 (Tesla T4)\n",
    "print(f\"\\nÄang sá»­ dá»¥ng: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e82ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 17 20:19:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GT 1030         Off |   00000000:18:00.0 Off |                  N/A |\n",
      "| 35%   35C    P8             N/A /   30W |     477MiB /   2048MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   43C    P0             34W /   70W |    2294MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1925      G   /usr/lib/xorg/Xorg                            285MiB |\n",
      "|    0   N/A  N/A      2531      G   /usr/bin/gnome-shell                           93MiB |\n",
      "|    0   N/A  N/A      2725      G   /opt/teamviewer/tv_bin/TeamViewer               7MiB |\n",
      "|    0   N/A  N/A      3094      G   ...irefox/6259/usr/lib/firefox/firefox         14MiB |\n",
      "|    0   N/A  N/A     12185      G   ...erProcess --variations-seed-version         57MiB |\n",
      "|    0   N/A  N/A     32837      G   gnome-control-center                            1MiB |\n",
      "|    0   N/A  N/A     52009      G   /snap/vlc/3777/usr/bin/vlc                      0MiB |\n",
      "|    0   N/A  N/A     52067      G   /snap/vlc/3777/usr/bin/vlc                      0MiB |\n",
      "|    1   N/A  N/A      1925      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3902834      C   ...iniconda3/envs/NguyenK56/bin/python       2286MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0653e",
   "metadata": {},
   "source": [
    "# Load model embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba58493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model tá»« á»• cá»©ng\n",
    "model_path = \"./Vietnamese_Embedding\"\n",
    "model = SentenceTransformer(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36d70d",
   "metadata": {},
   "source": [
    "# Táº¡o embeddings tá»« PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c5974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÄÆ°á»ng dáº«n file PDF báº¡n muá»‘n xá»­ lÃ½\n",
    "pdf_path = \"../data/file/CTKM_01.pdf\"\n",
    "output_dir = \"./results\"\n",
    "\n",
    "all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dbcd1",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: Check xem file pdf Ä‘Ã³ Ä‘Ã£ Ä‘Æ°á»£c embedding chÆ°a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd3d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pdf_embedded(path):\n",
    "    \"\"\"\n",
    "    Kiá»ƒm tra xem file PDF Ä‘Ã£ Ä‘Æ°á»£c embedding hay chÆ°a,\n",
    "    dá»±a vÃ o all_embeddings.pkl (danh sÃ¡ch cÃ¡c file Ä‘Ã£ xá»­ lÃ½).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        return False  # ChÆ°a cÃ³ dá»¯ liá»‡u chung => cháº¯c cháº¯n chÆ°a nhÃºng gÃ¬\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    existing_pdf_names = {entry['pdf_name'] for entry in all_data}\n",
    "\n",
    "    return pdf_name in existing_pdf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23587728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ PDF nÃ y Ä‘Ã£ Ä‘Æ°á»£c embedding trÆ°á»›c Ä‘Ã³.\n"
     ]
    }
   ],
   "source": [
    "if is_pdf_embedded(pdf_path):\n",
    "    print(\"ğŸ“Œ PDF nÃ y Ä‘Ã£ Ä‘Æ°á»£c embedding trÆ°á»›c Ä‘Ã³.\")\n",
    "else:\n",
    "    print(\"ğŸ”„ PDF nÃ y chÆ°a Ä‘Æ°á»£c embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087138e1",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 2: OCR PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ad984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Tiá»n xá»­ lÃ½ áº£nh Ä‘á»ƒ cáº£i thiá»‡n OCR\n",
    "    \"\"\"\n",
    "    if img.mode != 'L':\n",
    "        img = img.convert('L')\n",
    "\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(1.5)\n",
    "    img = img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d22f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdf_to_text(pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    OCR file PDF thÃ nh text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ“– Äang OCR file: {pdf_path}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "\n",
    "        full_text = \"\"\n",
    "        ocr_config = r'--oem 3 --psm 6 -l vie'\n",
    "\n",
    "        for page_num in range(total_pages):\n",
    "            print(f\"ğŸ”„ Xá»­ lÃ½ trang {page_num + 1}/{total_pages}...\")\n",
    "\n",
    "            page = doc.load_page(page_num)\n",
    "            matrix = fitz.Matrix(2.5, 2.5)\n",
    "            pix = page.get_pixmap(matrix=matrix)\n",
    "            img_data = pix.tobytes(\"png\")\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "            img = preprocess_image(img)\n",
    "\n",
    "            try:\n",
    "                page_text = pytesseract.image_to_string(img, config=ocr_config)\n",
    "                full_text += page_text.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Lá»—i OCR trang {page_num + 1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        print(f\"âœ… HoÃ n thÃ nh OCR {total_pages} trang\")\n",
    "\n",
    "        # ğŸ§  Táº¡o tÃªn file JSON theo tÃªn file PDF\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_ocr.txt\")\n",
    "\n",
    "        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # LÆ°u file JSON\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"ğŸ“„ Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o: {output_path}\")\n",
    "\n",
    "        # Tráº£ vá» danh sÃ¡ch cÃ¡c trang vá»›i ná»™i dung\n",
    "        return full_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i OCR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4e149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Äang OCR file: ../data/file/TTTS2025.pdf\n",
      "ğŸ”„ Xá»­ lÃ½ trang 1/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 2/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 3/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 4/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 5/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 6/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 7/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 8/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 9/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 10/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 11/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 12/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 13/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 14/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 15/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 16/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 17/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 18/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 19/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 20/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 21/22...\n",
      "ğŸ”„ Xá»­ lÃ½ trang 22/22...\n",
      "âœ… HoÃ n thÃ nh OCR 22 trang\n",
      "ğŸ“„ Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o: ./results/TTTS2025/TTTS2025_ocr.txt\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 1: OCR PDF\n",
    "raw_text = ocr_pdf_to_text(pdf_path, output_dir)\n",
    "if not raw_text:\n",
    "    print(\"âŒ KhÃ´ng thá»ƒ OCR file PDF. Vui lÃ²ng kiá»ƒm tra láº¡i.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb815a",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 3: LÃ m sáº¡ch text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    LÃ m sáº¡ch má»™t Ä‘oáº¡n vÄƒn báº£n OCR (string)\n",
    "    \"\"\"\n",
    "    # Loáº¡i kÃ½ tá»± khÃ´ng mong muá»‘n (giá»¯ láº¡i tiáº¿ng Viá»‡t, toÃ¡n há»c, Ä‘Æ¡n vá»‹)\n",
    "    text = re.sub(r'[^\\w\\s.,;:()\\[\\]?!\\\"\\'\\-â€“â€”â€¦Â°%â€°â‰¥â‰¤â†’â†â‰ =+/*<>\\n\\r]', '', text)\n",
    "\n",
    "    # Xá»­ lÃ½ lá»—i xuá»‘ng dÃ²ng giá»¯a tá»« hoáº·c giá»¯a cÃ¢u\n",
    "    text = re.sub(r'-\\n', '', text)             # ná»‘i tá»« bá»‹ gáº¡ch ná»‘i xuá»‘ng dÃ²ng\n",
    "    text = re.sub(r'\\n(?=\\w)', ' ', text)       # dÃ²ng xuá»‘ng khÃ´ng há»£p lÃ½ â†’ ná»‘i cÃ¢u\n",
    "\n",
    "    # Dáº¥u cháº¥m láº·p vÃ´ nghÄ©a â†’ ba cháº¥m\n",
    "    text = re.sub(r'\\.{3,}', '...', text)\n",
    "\n",
    "    # Chuáº©n hÃ³a khoáº£ng tráº¯ng\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)   # giá»¯ ngáº¯t Ä‘oáº¡n\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)       # nhiá»u khoáº£ng tráº¯ng â†’ 1 dáº¥u cÃ¡ch\n",
    "    text = re.sub(r' *\\n *', '\\n', text)      # bá» khoáº£ng tráº¯ng Ä‘áº§u/cuá»‘i dÃ²ng\n",
    "\n",
    "    # LÆ°u file\n",
    "    clean_text = text.strip()\n",
    "    \n",
    "    # ğŸ§  Táº¡o tÃªn file JSON theo tÃªn file PDF\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_clean.txt\")\n",
    "\n",
    "    # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # LÆ°u file JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"ğŸ“„ Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o: {output_path}\")\n",
    "    \n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e87f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ LÃ m sáº¡ch text...\n",
      "ğŸ“„ Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o: ./results/TTTS2025/TTTS2025_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 2: LÃ m sáº¡ch text\n",
    "print(\"ğŸ§¹ LÃ m sáº¡ch text...\")\n",
    "cleaned_text = clean_text(raw_text, pdf_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81300c",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 4: Chia thÃ nh chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7373f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sections(text):\n",
    "    \"\"\"\n",
    "    TÃ¡ch text thÃ nh cÃ¡c pháº§n theo tiÃªu Ä‘á» kiá»ƒu I., 1., a)\n",
    "    \"\"\"\n",
    "    sections = re.split(r'\\n(?=(?:[IVXLCDM]+\\.)|(?:\\d+\\.)|(?:[a-z]\\)))', text)\n",
    "    return [s.strip() for s in sections if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80143812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_chunks_vi_tokenized_with_section(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Chia vÄƒn báº£n tiáº¿ng Viá»‡t thÃ nh cÃ¡c chunk dá»±a trÃªn sá»‘ token,\n",
    "    giá»¯ nguyÃªn cáº¥u trÃºc section vÃ  cÃ¢u.\n",
    "    \"\"\"\n",
    "    sections = split_sections(text)\n",
    "    all_chunks = []\n",
    "\n",
    "    for section in sections:\n",
    "        sentences = sent_tokenize(section)\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            num_tokens = len(tokenizer.tokenize(sentence))\n",
    "\n",
    "            if current_tokens + num_tokens > chunk_size:\n",
    "                chunk_text = '\\n'.join(current_chunk).strip()\n",
    "                all_chunks.append(chunk_text)\n",
    "\n",
    "                # Overlap báº±ng token\n",
    "                overlap_chunk = []\n",
    "                total = 0\n",
    "                for s in reversed(current_chunk):\n",
    "                    toks = len(tokenizer.tokenize(s))\n",
    "                    if total + toks > overlap:\n",
    "                        break\n",
    "                    overlap_chunk.insert(0, s)\n",
    "                    total += toks\n",
    "\n",
    "                current_chunk = overlap_chunk + [sentence]\n",
    "                current_tokens = total + num_tokens\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += num_tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            all_chunks.append(' '.join(current_chunk).strip())\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e79362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Chia text thÃ nh chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ÄÃ£ táº¡o 56 chunks\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 3: Chia thÃ nh chunks\n",
    "print(\"âœ‚ï¸ Chia text thÃ nh chunks...\")\n",
    "chunks = split_text_to_chunks_vi_tokenized_with_section(cleaned_text)\n",
    "print(f\"ğŸ“ ÄÃ£ táº¡o {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af218cd6",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 5: Táº¡o embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a0b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Táº¡o embeddings cho cÃ¡c text chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ”„ Táº¡o embeddings cho {len(chunks)} chunks...\")\n",
    "        embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "        print(f\"âœ… HoÃ n thÃ nh táº¡o embeddings\")\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i táº¡o embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c83d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Táº¡o embeddings cho 56 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37664222005e45ddb7ff157bb9679308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HoÃ n thÃ nh táº¡o embeddings\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 4: Táº¡o embeddings\n",
    "embeddings = create_embeddings(chunks)\n",
    "if embeddings is None:\n",
    "    print(\"âŒ KhÃ´ng thá»ƒ táº¡o embeddings. Vui lÃ²ng kiá»ƒm tra láº¡i.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba621b9",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 6: LÆ°u embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6efd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(chunks, embeddings, pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    LÆ°u embeddings vÃ  chunks vÃ o file\n",
    "    \"\"\"\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i\n",
    "    os.makedirs(os.path.join(output_dir, pdf_name), exist_ok=True)\n",
    "\n",
    "    # LÆ°u dá»¯ liá»‡u\n",
    "    data = {\n",
    "        'pdf_name': pdf_name,\n",
    "        'chunks': chunks,\n",
    "        'embeddings': embeddings,\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # LÆ°u embeddings (pickle)\n",
    "    pickle_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_embeddings.pkl\")\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    # LÆ°u chunks (text file)\n",
    "    chunks_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_chunks.txt\")\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"CHUNKS Tá»ª FILE: {pdf_name}.pdf\\n\")\n",
    "        f.write(f\"Táº¡o lÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Tá»•ng sá»‘ chunks: {len(chunks)}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            f.write(f\"CHUNK {i}:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            f.write(chunk + \"\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\\n\")\n",
    "\n",
    "    # LÆ°u thÃ´ng tin embeddings (text file)\n",
    "    embedding_info_path = os.path.join(\n",
    "        output_dir, pdf_name, f\"{pdf_name}_embedding_info.txt\")\n",
    "    with open(embedding_info_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"THÃ”NG TIN EMBEDDINGS: {pdf_name}.pdf\\n\")\n",
    "        f.write(f\"Táº¡o lÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"ğŸ“Š THá»NG KÃŠ:\\n\")\n",
    "        f.write(f\"- Tá»•ng sá»‘ chunks: {len(chunks)}\\n\")\n",
    "        f.write(f\"- KÃ­ch thÆ°á»›c embeddings: {embeddings.shape}\\n\")\n",
    "        f.write(f\"- Kiá»ƒu dá»¯ liá»‡u: {embeddings.dtype}\\n\")\n",
    "        f.write(\n",
    "            f\"- KÃ­ch thÆ°á»›c má»—i vector: {embeddings.shape[1]} dimensions\\n\\n\")\n",
    "\n",
    "        f.write(f\"ğŸ“ PREVIEW EMBEDDINGS (5 chunks Ä‘áº§u):\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "        for i in range(min(5, len(chunks))):\n",
    "            f.write(f\"\\nCHUNK {i+1}:\\n\")\n",
    "            f.write(f\"Text: {chunks[i][:100]}...\\n\")\n",
    "            f.write(\n",
    "                f\"Embedding vector (10 giÃ¡ trá»‹ Ä‘áº§u): {embeddings[i][:10].tolist()}\\n\")\n",
    "            f.write(f\"Vector norm: {np.linalg.norm(embeddings[i]):.4f}\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    # 4ï¸âƒ£ LÆ°u FAISS index\n",
    "    index_path = os.path.join(output_dir, pdf_name, f\"{pdf_name}_faiss.index\")\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # --- ğŸ” Cáº­p nháº­t FAISS chung ---\n",
    "    all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "    if os.path.exists(all_faiss_path):\n",
    "        index_all = faiss.read_index(all_faiss_path)\n",
    "    else:\n",
    "        index_all = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index_all.add(embeddings.astype(np.float32))\n",
    "    faiss.write_index(index_all, all_faiss_path)\n",
    "\n",
    "    # --- ğŸ” Cáº­p nháº­t pickle chung ---\n",
    "    all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")\n",
    "    if os.path.exists(all_pickle_path):\n",
    "        with open(all_pickle_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "    else:\n",
    "        all_data = []\n",
    "\n",
    "    all_data.append(data)\n",
    "\n",
    "    with open(all_pickle_path, 'wb') as f:\n",
    "        pickle.dump(all_data, f)\n",
    "\n",
    "    print(f\"ğŸ’¾ ÄÃ£ lÆ°u embeddings: {pickle_path}\")\n",
    "    print(f\"ğŸ“„ ÄÃ£ lÆ°u chunks: {chunks_path}\")\n",
    "    print(f\"ğŸ“Š ÄÃ£ lÆ°u thÃ´ng tin embeddings: {embedding_info_path}\")\n",
    "    print(f\"ğŸ“Œ ÄÃ£ lÆ°u FAISS index: {index_path}\")\n",
    "    print(f\"ğŸ” Cáº­p nháº­t FAISS chung: {all_faiss_path}\")\n",
    "    print(f\"ğŸ“¦ Cáº­p nháº­t pickle chung: {all_pickle_path}\")\n",
    "\n",
    "    return pickle_path, index_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b07fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ÄÃ£ lÆ°u embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "ğŸ“„ ÄÃ£ lÆ°u chunks: ./results/TTTS2025/TTTS2025_chunks.txt\n",
      "ğŸ“Š ÄÃ£ lÆ°u thÃ´ng tin embeddings: ./results/TTTS2025/TTTS2025_embedding_info.txt\n",
      "ğŸ“Œ ÄÃ£ lÆ°u FAISS index: ./results/TTTS2025/TTTS2025_faiss.index\n",
      "ğŸ” Cáº­p nháº­t FAISS chung: ./results/all_faiss.index\n",
      "ğŸ“¦ Cáº­p nháº­t pickle chung: ./results/all_embeddings.pkl\n",
      "\n",
      "ğŸ‰ HOÃ€N THÃ€NH!\n",
      "ğŸ“Š Thá»‘ng kÃª:\n",
      "   - Sá»‘ chunks: 56\n",
      "   - KÃ­ch thÆ°á»›c embedding: (56, 1024)\n",
      "   - File embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "   - File chunks: ./results/TTTS2025/TTTS2025_chunks.txt\n",
      "   - File thÃ´ng tin: ./results/TTTS2025/TTTS2025_embedding_info.txt\n",
      "âœ… ÄÆ°á»ng dáº«n embeddings: ./results/TTTS2025/TTTS2025_embeddings.pkl\n",
      "âœ… ÄÆ°á»ng dáº«n FAISS index: ./results/TTTS2025/TTTS2025_faiss.index\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 5: LÆ°u embeddings\n",
    "pickle_path, faiss_path = save_embeddings(chunks, embeddings, pdf_path, output_dir)\n",
    "\n",
    "print(f\"\\nğŸ‰ HOÃ€N THÃ€NH!\")\n",
    "print(f\"ğŸ“Š Thá»‘ng kÃª:\")\n",
    "print(f\"   - Sá»‘ chunks: {len(chunks)}\")\n",
    "print(f\"   - KÃ­ch thÆ°á»›c embedding: {embeddings.shape}\")\n",
    "print(f\"   - File embeddings: {pickle_path}\")\n",
    "print(\n",
    "    f\"   - File chunks: {pickle_path.replace('_embeddings.pkl', '_chunks.txt')}\")\n",
    "print(\n",
    "    f\"   - File thÃ´ng tin: {pickle_path.replace('_embeddings.pkl', '_embedding_info.txt')}\")\n",
    "\n",
    "# LÆ°u láº¡i Ä‘Æ°á»ng dáº«n file .pkl Ä‘á»ƒ sá»­ dá»¥ng sau\n",
    "print(f\"âœ… ÄÆ°á»ng dáº«n embeddings: {pickle_path}\")\n",
    "print(f\"âœ… ÄÆ°á»ng dáº«n FAISS index: {faiss_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9f62",
   "metadata": {},
   "source": [
    "## XÃ³a pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d772b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XÃ³a thÆ° má»¥c chá»©a embeddings cá»§a file PDF\n",
    "def delete_pdf_folder(pdf_path, output_dir):\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    folder_path = os.path.join(output_dir, pdf_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"ğŸ—‘ï¸ ÄÃ£ xoÃ¡ thÆ° má»¥c: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14416ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XÃ³a khá»i all_embeddings.pkl\n",
    "def remove_from_all_embeddings(pdf_path, all_pickle_path):\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        print(\"âš ï¸ all_embeddings.pkl khÃ´ng tá»“n táº¡i.\")\n",
    "        return\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    new_data = [entry for entry in all_data if entry['pdf_name'] != pdf_name]\n",
    "\n",
    "    if len(new_data) == len(all_data):\n",
    "        print(f\"âŒ PDF '{pdf_name}' khÃ´ng cÃ³ trong all_embeddings.pkl.\")\n",
    "    else:\n",
    "        with open(all_pickle_path, 'wb') as f:\n",
    "            pickle.dump(new_data, f)\n",
    "        print(f\"âœ… ÄÃ£ xoÃ¡ '{pdf_name}' khá»i all_embeddings.pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d13b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XÃ³a khá»i all_faiss.index\n",
    "def rebuild_faiss_from_pickle(all_pickle_path, all_faiss_path):\n",
    "    if not os.path.exists(all_pickle_path):\n",
    "        print(\"âš ï¸ all_embeddings.pkl khÃ´ng tá»“n táº¡i.\")\n",
    "        return\n",
    "\n",
    "    with open(all_pickle_path, 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"âš ï¸ KhÃ´ng cÃ²n dá»¯ liá»‡u nÃ o trong all_embeddings.pkl.\")\n",
    "        if os.path.exists(all_faiss_path):\n",
    "            os.remove(all_faiss_path)\n",
    "            print(\"ğŸ—‘ï¸ ÄÃ£ xoÃ¡ all_faiss.index rá»—ng.\")\n",
    "        return\n",
    "\n",
    "    # Gom táº¥t cáº£ vectors láº¡i\n",
    "    all_vectors = np.vstack([entry['embeddings'] for entry in all_data]).astype(np.float32)\n",
    "    dim = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(all_vectors)\n",
    "    faiss.write_index(index, all_faiss_path)\n",
    "    print(f\"ğŸ”„ ÄÃ£ xÃ¢y láº¡i all_faiss.index vá»›i {all_vectors.shape[0]} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pdf_folder_and_update(pdf_path, output_dir):\n",
    "    all_pickle_path = os.path.join(output_dir, \"all_embeddings.pkl\")\n",
    "    all_faiss_path = os.path.join(output_dir, \"all_faiss.index\")\n",
    "    \n",
    "    delete_pdf_folder(pdf_path, output_dir)\n",
    "    remove_from_all_embeddings(pdf_path, all_pickle_path)\n",
    "    rebuild_faiss_from_pickle(all_pickle_path, all_faiss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fce571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ xoÃ¡ 'CTKM_01' khá»i all_embeddings.pkl.\n",
      "ğŸ”„ ÄÃ£ xÃ¢y láº¡i all_faiss.index vá»›i 56 vectors.\n"
     ]
    }
   ],
   "source": [
    "delete_pdf_folder_and_update(pdf_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbb63f",
   "metadata": {},
   "source": [
    "# Demo tÃ¬m kiáº¿m vá»›i cÃ¢u há»i báº¥t ká»³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6effc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thá»­ tÃ¬m kiáº¿m vá»›i cÃ¢u há»i báº¥t ká»³\n",
    "query = \"Trá»¥ sá»Ÿ cá»§a trÆ°á»ng náº±m á»Ÿ Ä‘Ã¢u?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e58a2a",
   "metadata": {},
   "source": [
    "## DÃ¹ng Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42997699",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "493169b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path):\n",
    "    \"\"\"\n",
    "    Táº£i embeddings tá»« file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(embeddings_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f\"ğŸ“‚ ÄÃ£ táº£i embeddings tá»«: {embeddings_path}\")\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i táº£i embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618ea1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ÄÃ£ táº£i embeddings tá»«: ./results/TTTS2025_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Táº£i láº¡i dá»¯ liá»‡u embeddings Ä‘Ã£ táº¡o\n",
    "data = load_embeddings(pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11f0c8",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 2: TÃ¬m kiáº¿m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec0e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_chunks(query, embeddings_data, top_k=3):\n",
    "    \"\"\"\n",
    "    TÃ¬m cÃ¡c chunk tÆ°Æ¡ng tá»± vá»›i query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Táº¡o embedding cho query\n",
    "        query_embedding = model.encode([query])\n",
    "\n",
    "        # TÃ­nh cosine similarity\n",
    "        similarities = np.dot(\n",
    "            embeddings_data['embeddings'], query_embedding.T).flatten()\n",
    "\n",
    "        # Láº¥y top_k káº¿t quáº£\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'chunk': embeddings_data['chunks'][idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'index': idx\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i tÃ¬m kiáº¿m: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05b86080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” DEMO TÃŒM KIáº¾M\n",
      "ğŸ“š PDF: TTTS2025\n",
      "ğŸ“ Sá»‘ chunks: 56\n",
      "==================================================\n",
      "\n",
      "1. ğŸ” Äá»™ tÆ°Æ¡ng Ä‘á»“ng: 0.3401\n",
      "ğŸ“„ Ná»™i dung: + ThÃ¡ng 01 nÄƒm 2008, Há»c viá»‡n KTQS Ä‘Æ°á»£c NhÃ  nÆ°á»›c cÃ´ng nháº­n lÃ  má»™t trong 15 trÆ°á»ng Äáº¡i há»c trá»ng Ä‘iÃªm Quá»‘c gia.\n",
      "b) Sá»© má»‡nh Sá»© má»‡nh cá»§a.\n",
      "Há»c viá»‡n KTQS lÃ  Ä‘Ã o táº¡o nguá»“n nhÃ¢n lá»±c cháº¥t lÆ°á»£ng cao, nghiÃªn cá»©u phÃ¡t triÃªn, sáº£n xuÃ¢t chÃª thá»­, chuyÃªn giao cÃ´ng nghá»‡ tiÃªn tiÃªn vÃ  há»™i nháº­p quá»‘c tÃª, gÃ³p phÃ¢n Ä‘áº¯c lá»±...\n",
      "\n",
      "2. ğŸ” Äá»™ tÆ°Æ¡ng Ä‘á»“ng: 0.2782\n",
      "ğŸ“„ Ná»™i dung: 1. TÃªn cÆ¡ sá»Ÿ Ä‘Ã o táº¡o: Há»c viá»‡n Ká»¹ thuáº­t quÃ¢n sá»± (MIilitary Technical Academy), tÃªn dÃ¢n sá»± lÃ  TrÆ°á»ng Äáº¡i há»c Ká»¹ thuáº­t LÃª QuÃ½ ÄÃ´n (Le Quy Äon Technical University)...\n",
      "\n",
      "3. ğŸ” Äá»™ tÆ°Æ¡ng Ä‘á»“ng: 0.2573\n",
      "ğŸ“„ Ná»™i dung: Sá»‘ Ä‘iá»‡n thoáº¡i liÃªn há»‡: 069.5 15.2263 6. Äá»‹a chá»‰ cÃ´ng khai quy cháº¿ tuyá»ƒn sinh, thÃ´ng tin tuyá»ƒn sinh: http:/www.mta.edu.vn; 7. CÃ¡c thÃ´ng tin cÃ´ng khai vá» hoáº¡t Ä‘á»™ng cá»§a Há»c viá»‡n: Äá»‹a chá»‰ cÃ´ng khai thÃ´ng tin: http://www.mta.edu.vn; 4) NgÃ nh Ä‘Ã o táº¡o Há»c viá»‡n tá»• chá»©c Ä‘áº£o táº¡o trÃ¬nh Ä‘á»™ Ä‘áº¡i há»c vá»›i 15 ngÃ nh ...\n"
     ]
    }
   ],
   "source": [
    "results = find_similar_chunks(query, data, top_k=3)\n",
    "\n",
    "print(f\"\\nğŸ” DEMO TÃŒM KIáº¾M\")\n",
    "print(f\"ğŸ“š PDF: {data['pdf_name']}\")\n",
    "print(f\"ğŸ“ Sá»‘ chunks: {len(data['chunks'])}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# In káº¿t quáº£\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ğŸ” Äá»™ tÆ°Æ¡ng Ä‘á»“ng: {result['similarity']:.4f}\")\n",
    "    print(f\"ğŸ“„ Ná»™i dung: {result['chunk'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc8236",
   "metadata": {},
   "source": [
    "## DÃ¹ng FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45b3f8",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: Load FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4ce55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index(index_path):\n",
    "    print(f\"ğŸ“‚ Äang táº£i FAISS index tá»«: {index_path}\")\n",
    "    return faiss.read_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd87e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang táº£i FAISS index tá»«: ./results/TTTS2025_faiss.index\n"
     ]
    }
   ],
   "source": [
    "index = load_faiss_index(faiss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a5ebe",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 2: Encode cÃ¢u há»i thÃ nh vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query):\n",
    "    return model.encode([query])  # tráº£ vá» numpy array (1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c44eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = encode_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690934c8",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 3: Truy váº¥n FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5615d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(index, query_vector, top_k=5):\n",
    "    D, I = index.search(query_vector.astype('float32'), top_k)\n",
    "    return I[0], D[0]  # indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "indices, distances = search_faiss(index, query_vector, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cdbac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Káº¿t quáº£ truy váº¥n:\n",
      "\n",
      "ğŸ”¹ Káº¿t quáº£ 1: (score: 1.3199)\n",
      "+ ThÃ¡ng 01 nÄƒm 2008, Há»c viá»‡n KTQS Ä‘Æ°á»£c NhÃ  nÆ°á»›c cÃ´ng nháº­n lÃ  má»™t trong 15 trÆ°á»ng Äáº¡i há»c trá»ng Ä‘iÃªm Quá»‘c gia.\n",
      "b) Sá»© má»‡nh Sá»© má»‡nh cá»§a.\n",
      "Há»c viá»‡n KTQS lÃ  Ä‘Ã o táº¡o nguá»“n nhÃ¢n lá»±c cháº¥t lÆ°á»£ng cao, nghiÃªn cá»©u phÃ¡t triÃªn, sáº£n xuÃ¢t chÃª thá»­, chuyÃªn giao cÃ´ng nghá»‡ tiÃªn tiÃªn vÃ  há»™i nháº­p quá»‘c tÃª, gÃ³p phÃ¢n Ä‘áº¯c lá»±c vÃ o sá»± nghiá»‡p xÃ¢y dá»±ng vÃ  báº£o vá»‡ Tá»• quá»‘c, phÃ¡t triÃªn ngÃ nh khoa há»c cÃ´ng nghá»‡ quÃ¢n sá»± Viá»‡t Nam.\n",
      "c) TÃ¢m nhÃ¬n\n",
      "_ Äáº¿n nÄƒm 2030, Há»c viá»‡n KTQS trá»Ÿ thÃ nh trÆ°á»ng Äáº¡i há»c nghiÃªn cá»©u nÄƒm trong tá»‘p Ä‘Ã¢u vá» Kho\n",
      "\n",
      "ğŸ”¹ Káº¿t quáº£ 2: (score: 1.4436)\n",
      "1. TÃªn cÆ¡ sá»Ÿ Ä‘Ã o táº¡o: Há»c viá»‡n Ká»¹ thuáº­t quÃ¢n sá»± (MIilitary Technical Academy), tÃªn dÃ¢n sá»± lÃ  TrÆ°á»ng Äáº¡i há»c Ká»¹ thuáº­t LÃª QuÃ½ ÄÃ´n (Le Quy Äon Technical University)\n",
      "\n",
      "ğŸ”¹ Káº¿t quáº£ 3: (score: 1.4854)\n",
      "Sá»‘ Ä‘iá»‡n thoáº¡i liÃªn há»‡: 069.5 15.2263 6. Äá»‹a chá»‰ cÃ´ng khai quy cháº¿ tuyá»ƒn sinh, thÃ´ng tin tuyá»ƒn sinh: http:/www.mta.edu.vn; 7. CÃ¡c thÃ´ng tin cÃ´ng khai vá» hoáº¡t Ä‘á»™ng cá»§a Há»c viá»‡n: Äá»‹a chá»‰ cÃ´ng khai thÃ´ng tin: http://www.mta.edu.vn; 4) NgÃ nh Ä‘Ã o táº¡o Há»c viá»‡n tá»• chá»©c Ä‘áº£o táº¡o trÃ¬nh Ä‘á»™ Ä‘áº¡i há»c vá»›i 15 ngÃ nh gáº¯n vá»›i cÃ¡c lÄ©nh vá»±c phÃ¡t triÃªn cá»§a khoa há»c vÃ  cÃ´ng nghá»‡ nÃ³i chung vÃ  khoa há»c cÃ´ng nghá»‡ quÃ¢n sá»± nÃ³i riÃªng bao gá»“m: TT TÃªn ngÃ nh Ghi chÃº 1_ Ká»¹ thuáº­t CÆ¡ khÃ­ 7520103 2_ÃŒ Ká»¹ thuáº­t Äiá»‡n tá»­ - viá»…n thÃ´ng 7\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Œ Káº¿t quáº£ truy váº¥n:\")\n",
    "for i, (idx, dist) in enumerate(zip(indices, distances), 1):\n",
    "    print(f\"\\nğŸ”¹ Káº¿t quáº£ {i}: (score: {dist:.4f})\")\n",
    "    print(chunks[idx][:500])  # in tá»‘i Ä‘a 500 kÃ½ tá»±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NguyenK56",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
